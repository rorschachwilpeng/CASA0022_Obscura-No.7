#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Obscura No.7 æ€§èƒ½å’Œå‹åŠ›æµ‹è¯•å¥—ä»¶
æµ‹è¯•ç³»ç»Ÿåœ¨å„ç§æ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°å’Œç¨³å®šæ€§
"""

import requests
import json
import time
import threading
import concurrent.futures
import random
import statistics
from datetime import datetime
from typing import Dict, List, Any
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('performance_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, base_url: str = "https://casa0022-obscura-no-7.onrender.com"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'Obscura-Performance-Test/1.0'
        })
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.response_times = []
        self.error_count = 0
        self.success_count = 0
        
    def create_test_payload(self, lat: float = None, lon: float = None) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•è´Ÿè½½æ•°æ®"""
        if lat is None:
            lat = random.uniform(-85, 85)  # æœ‰æ•ˆçº¬åº¦èŒƒå›´
        if lon is None:
            lon = random.uniform(-180, 180)  # æœ‰æ•ˆç»åº¦èŒƒå›´
            
        return {
            "environmental_data": {
                "latitude": lat,
                "longitude": lon,
                "temperature": round(random.uniform(-20.0, 45.0), 1),
                "humidity": random.randint(10, 100),
                "pressure": random.randint(950, 1050),
                "wind_speed": round(random.uniform(0.0, 25.0), 1),
                "weather_description": random.choice([
                    "clear sky", "few clouds", "scattered clouds", 
                    "overcast clouds", "light rain", "moderate rain",
                    "heavy rain", "snow", "fog", "thunderstorm"
                ]),
                "location_name": f"Test Location {random.randint(1, 1000)}"
            },
            "hours_ahead": random.choice([6, 12, 24, 48, 72, 96, 120, 168])
        }
    
    def single_api_request(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå•æ¬¡APIè¯·æ±‚"""
        start_time = time.time()
        
        try:
            response = self.session.post(
                f"{self.base_url}/api/v1/ml/predict",
                json=payload,
                timeout=30
            )
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                self.success_count += 1
                self.response_times.append(response_time)
                return {
                    "success": True,
                    "response_time": response_time,
                    "prediction": result.get("prediction", {}),
                    "status_code": response.status_code
                }
            else:
                self.error_count += 1
                return {
                    "success": False,
                    "response_time": response_time,
                    "error": f"HTTP {response.status_code}",
                    "status_code": response.status_code
                }
                
        except Exception as e:
            response_time = time.time() - start_time
            self.error_count += 1
            return {
                "success": False,
                "response_time": response_time,
                "error": str(e),
                "status_code": 0
            }
    
    def response_time_benchmark(self, num_requests: int = 20) -> Dict[str, Any]:
        """APIå“åº”æ—¶é—´åŸºå‡†æµ‹è¯•"""
        logger.info(f"ğŸƒ å¼€å§‹å“åº”æ—¶é—´åŸºå‡†æµ‹è¯• - {num_requests}æ¬¡è¯·æ±‚")
        
        # é‡ç½®è®¡æ•°å™¨
        self.response_times = []
        self.success_count = 0
        self.error_count = 0
        
        results = []
        start_time = time.time()
        
        for i in range(num_requests):
            payload = self.create_test_payload()
            result = self.single_api_request(payload)
            results.append(result)
            
            if (i + 1) % 5 == 0:
                logger.info(f"   å®Œæˆ {i + 1}/{num_requests} è¯·æ±‚")
            
            # è¯·æ±‚é—´çŸ­æš‚é—´éš”
            time.sleep(0.2)
        
        total_time = time.time() - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        if self.response_times:
            avg_response_time = statistics.mean(self.response_times)
            median_response_time = statistics.median(self.response_times)
            min_response_time = min(self.response_times)
            max_response_time = max(self.response_times)
        else:
            avg_response_time = median_response_time = min_response_time = max_response_time = 0
        
        success_rate = (self.success_count / num_requests) * 100
        
        benchmark_result = {
            "test_type": "response_time_benchmark",
            "total_requests": num_requests,
            "successful_requests": self.success_count,
            "failed_requests": self.error_count,
            "success_rate": f"{success_rate:.1f}%",
            "total_test_time": f"{total_time:.2f}s",
            "requests_per_second": f"{num_requests / total_time:.2f}",
            "response_time_stats": {
                "average": f"{avg_response_time:.3f}s",
                "median": f"{median_response_time:.3f}s",
                "min": f"{min_response_time:.3f}s",
                "max": f"{max_response_time:.3f}s"
            },
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"âœ… åŸºå‡†æµ‹è¯•å®Œæˆ - æˆåŠŸç‡: {success_rate:.1f}%, å¹³å‡å“åº”: {avg_response_time:.3f}s")
        
        return benchmark_result
    
    def concurrent_load_test(self, num_concurrent: int = 5, requests_per_thread: int = 3) -> Dict[str, Any]:
        """å¹¶å‘è´Ÿè½½æµ‹è¯•"""
        logger.info(f"ğŸ”„ å¼€å§‹å¹¶å‘è´Ÿè½½æµ‹è¯• - {num_concurrent}å¹¶å‘çº¿ç¨‹, æ¯çº¿ç¨‹{requests_per_thread}è¯·æ±‚")
        
        def worker_thread(thread_id: int) -> List[Dict[str, Any]]:
            """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
            thread_results = []
            for i in range(requests_per_thread):
                payload = self.create_test_payload()
                result = self.single_api_request(payload)
                result["thread_id"] = thread_id
                result["request_id"] = i
                thread_results.append(result)
                time.sleep(0.5)  # çº¿ç¨‹å†…è¯·æ±‚é—´éš”
            return thread_results
        
        start_time = time.time()
        all_results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘è¯·æ±‚
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent) as executor:
            future_to_thread = {executor.submit(worker_thread, i): i for i in range(num_concurrent)}
            
            for future in concurrent.futures.as_completed(future_to_thread):
                thread_id = future_to_thread[future]
                try:
                    thread_results = future.result()
                    all_results.extend(thread_results)
                    logger.info(f"   çº¿ç¨‹ {thread_id} å®Œæˆ")
                except Exception as e:
                    logger.error(f"   çº¿ç¨‹ {thread_id} å¼‚å¸¸: {e}")
        
        total_time = time.time() - start_time
        total_requests = num_concurrent * requests_per_thread
        
        # åˆ†æç»“æœ
        successful_results = [r for r in all_results if r["success"]]
        failed_results = [r for r in all_results if not r["success"]]
        
        if successful_results:
            response_times = [r["response_time"] for r in successful_results]
            avg_response_time = statistics.mean(response_times)
            max_response_time = max(response_times)
            min_response_time = min(response_times)
        else:
            avg_response_time = max_response_time = min_response_time = 0
        
        success_rate = (len(successful_results) / total_requests) * 100
        
        concurrent_result = {
            "test_type": "concurrent_load_test",
            "concurrent_threads": num_concurrent,
            "requests_per_thread": requests_per_thread,
            "total_requests": total_requests,
            "successful_requests": len(successful_results),
            "failed_requests": len(failed_results),
            "success_rate": f"{success_rate:.1f}%",
            "total_test_time": f"{total_time:.2f}s",
            "requests_per_second": f"{total_requests / total_time:.2f}",
            "concurrent_response_time": {
                "average": f"{avg_response_time:.3f}s",
                "min": f"{min_response_time:.3f}s",
                "max": f"{max_response_time:.3f}s"
            },
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"âœ… å¹¶å‘æµ‹è¯•å®Œæˆ - æˆåŠŸç‡: {success_rate:.1f}%, å¹³å‡å“åº”: {avg_response_time:.3f}s")
        
        return concurrent_result
    
    def boundary_condition_test(self) -> Dict[str, Any]:
        """è¾¹ç•Œæ¡ä»¶æµ‹è¯•"""
        logger.info("ğŸ” å¼€å§‹è¾¹ç•Œæ¡ä»¶æµ‹è¯•")
        
        boundary_tests = [
            # æç«¯åæ ‡
            {"name": "åŒ—æç‚¹", "lat": 89, "lon": 0},
            {"name": "å—æç‚¹", "lat": -89, "lon": 0},
            {"name": "å›½é™…æ—¥æœŸå˜æ›´çº¿", "lat": 0, "lon": 180},
            {"name": "æœ¬åˆå­åˆçº¿", "lat": 0, "lon": 0},
            {"name": "æç«¯è¥¿ç»", "lat": 0, "lon": -180},
        ]
        
        results = []
        
        for test_case in boundary_tests:
            logger.info(f"   æµ‹è¯• {test_case['name']}: ({test_case['lat']}, {test_case['lon']})")
            
            payload = self.create_test_payload(test_case["lat"], test_case["lon"])
            result = self.single_api_request(payload)
            
            result["test_case"] = test_case["name"]
            result["coordinates"] = (test_case["lat"], test_case["lon"])
            results.append(result)
            
            time.sleep(1)  # é—´éš”è¯·æ±‚
        
        successful_boundary_tests = sum(1 for r in results if r["success"])
        boundary_success_rate = (successful_boundary_tests / len(boundary_tests)) * 100
        
        boundary_result = {
            "test_type": "boundary_condition_test",
            "total_boundary_tests": len(boundary_tests),
            "successful_tests": successful_boundary_tests,
            "failed_tests": len(boundary_tests) - successful_boundary_tests,
            "boundary_success_rate": f"{boundary_success_rate:.1f}%",
            "test_results": results,
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"âœ… è¾¹ç•Œæ¡ä»¶æµ‹è¯•å®Œæˆ - æˆåŠŸç‡: {boundary_success_rate:.1f}%")
        
        return boundary_result
    
    def run_performance_test(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹Obscura No.7æ€§èƒ½æµ‹è¯•")
        suite_start_time = time.time()
        
        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        benchmark_result = self.response_time_benchmark(20)
        time.sleep(2)  # æµ‹è¯•é—´éš”
        
        concurrent_result = self.concurrent_load_test(5, 3)
        time.sleep(2)  # æµ‹è¯•é—´éš”
        
        boundary_result = self.boundary_condition_test()
        
        suite_total_time = time.time() - suite_start_time
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        performance_report = {
            "test_suite": "Obscura No.7 Performance Test",
            "test_timestamp": datetime.now().isoformat(),
            "total_suite_time": f"{suite_total_time:.2f}s",
            "test_results": {
                "response_time_benchmark": benchmark_result,
                "concurrent_load_test": concurrent_result,
                "boundary_condition_test": boundary_result
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        self.save_performance_report(performance_report)
        
        # æ‰“å°æ‘˜è¦
        self.print_performance_summary(performance_report)
        
        return performance_report
    
    def save_performance_report(self, report: Dict[str, Any]):
        """ä¿å­˜æ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_test_report_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ“„ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
    
    def print_performance_summary(self, report: Dict[str, Any]):
        """æ‰“å°æ€§èƒ½æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ”­ OBSCURA NO.7 æ€§èƒ½æµ‹è¯•æ‘˜è¦")
        print("="*80)
        print(f"â±ï¸  æ€»æµ‹è¯•æ—¶é—´: {report['total_suite_time']}")
        print("")
        
        # è¯¦ç»†ç»“æœ
        results = report["test_results"]
        
        print("ğŸ“ˆ æµ‹è¯•ç»“æœè¯¦æƒ…:")
        print(f"  å“åº”æ—¶é—´åŸºå‡†: {results['response_time_benchmark']['success_rate']} æˆåŠŸ")
        print(f"    - å¹³å‡å“åº”: {results['response_time_benchmark']['response_time_stats']['average']}")
        print(f"    - æœ€å¿«å“åº”: {results['response_time_benchmark']['response_time_stats']['min']}")
        print(f"    - æœ€æ…¢å“åº”: {results['response_time_benchmark']['response_time_stats']['max']}")
        
        print(f"  å¹¶å‘è´Ÿè½½æµ‹è¯•: {results['concurrent_load_test']['success_rate']} æˆåŠŸ")
        print(f"    - å¹¶å‘çº¿ç¨‹: {results['concurrent_load_test']['concurrent_threads']}")
        print(f"    - è¯·æ±‚/ç§’: {results['concurrent_load_test']['requests_per_second']}")
        
        print(f"  è¾¹ç•Œæ¡ä»¶æµ‹è¯•: {results['boundary_condition_test']['boundary_success_rate']} æˆåŠŸ")
        
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”­ Obscura No.7 æ€§èƒ½å’Œå‹åŠ›æµ‹è¯•å¥—ä»¶")
    print("æµ‹è¯•ç³»ç»Ÿæ€§èƒ½è¡¨ç°å’Œç¨³å®šæ€§\n")
    
    # åˆ›å»ºæ€§èƒ½æµ‹è¯•å¥—ä»¶
    performance_test = PerformanceTestSuite()
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    report = performance_test.run_performance_test()
    
    print("ğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    return 0

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code) 