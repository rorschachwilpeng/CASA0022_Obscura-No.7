#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
66ç‰¹å¾æ¨¡å‹è®­ç»ƒè„šæœ¬ - ä¼˜åŒ–ç‰ˆæœ¬
- å®æ—¶è¿›åº¦æ¡æ˜¾ç¤ºæ•°æ®æ”¶é›†è¿›åº¦
- æ•°æ®æ”¶é›†å®Œæˆåç«‹å³ä¿å­˜ï¼Œè®­ç»ƒå‰åŠ è½½
- æ”¯æŒå°è§„æ¨¡éªŒè¯æ¨¡å¼
"""

import sys
import os
import numpy as np
import pandas as pd
from datetime import datetime
import joblib
import json
import pickle
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# è¿›åº¦æ¡
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_dir))))
api_dir = os.path.join(project_root, 'api')
sys.path.insert(0, project_root)
sys.path.insert(0, api_dir)

# MLç›¸å…³åº“
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler

# æ·±åº¦å­¦ä¹ ç›¸å…³åº“
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    TF_AVAILABLE = True
    print("âœ… TensorFlowå¯ç”¨")
except ImportError:
    TF_AVAILABLE = False
    print("âŒ TensorFlowä¸å¯ç”¨ï¼Œå°†è·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹")

from utils.simplified_feature_engineer import get_simplified_feature_engineer

class OptimizedModelTrainer:
    """ä¼˜åŒ–çš„66ç‰¹å¾æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, output_dir: str = "trained_models_66", cache_dir: str = "training_data_cache"):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.output_dir = output_dir
        self.cache_dir = cache_dir
        self.feature_engineer = get_simplified_feature_engineer()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.cache_dir, exist_ok=True)
        
        print(f"OptimizedModelTraineråˆå§‹åŒ–å®Œæˆ")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def get_cache_filename(self, n_samples: int) -> str:
        """æ ¹æ®æ ·æœ¬æ•°ç”Ÿæˆç¼“å­˜æ–‡ä»¶å"""
        return os.path.join(self.cache_dir, f"training_data_{n_samples}_samples.pkl")
    
    def load_cached_data(self, n_samples: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ä»ç¼“å­˜åŠ è½½è®­ç»ƒæ•°æ®"""
        cache_file = self.get_cache_filename(n_samples)
        
        if os.path.exists(cache_file):
            print(f"ğŸ“ å‘ç°ç¼“å­˜æ•°æ®: {cache_file}")
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)
                
                X = cached_data['X']
                y_climate = cached_data['y_climate']
                y_geographic = cached_data['y_geographic']
                
                print(f"âœ… ç¼“å­˜æ•°æ®åŠ è½½æˆåŠŸ:")
                print(f"   ç‰¹å¾å½¢çŠ¶: {X.shape}")
                print(f"   Climateæ ‡ç­¾èŒƒå›´: [{y_climate.min():.3f}, {y_climate.max():.3f}]")
                print(f"   Geographicæ ‡ç­¾èŒƒå›´: [{y_geographic.min():.3f}, {y_geographic.max():.3f}]")
                print(f"   ç¼“å­˜æ—¶é—´: {cached_data.get('timestamp', 'Unknown')}")
                
                return X, y_climate, y_geographic
                
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜æ•°æ®åŠ è½½å¤±è´¥: {e}")
                print("å°†é‡æ–°ç”Ÿæˆæ•°æ®...")
                return None, None, None
        else:
            print(f"ğŸ“‚ æœªå‘ç°{n_samples}æ ·æœ¬çš„ç¼“å­˜æ•°æ®ï¼Œå°†é‡æ–°ç”Ÿæˆ...")
            return None, None, None
    
    def save_data_to_cache(self, X: np.ndarray, y_climate: np.ndarray, y_geographic: np.ndarray, n_samples: int):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        cache_file = self.get_cache_filename(n_samples)
        
        try:
            cache_data = {
                'X': X,
                'y_climate': y_climate,
                'y_geographic': y_geographic,
                'timestamp': datetime.now().isoformat(),
                'n_samples': X.shape[0],
                'n_features': X.shape[1],
                'sampling_strategy': 'hybrid_70_30'
            }
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            print(f"ğŸ’¾ æ•°æ®å·²ç¼“å­˜åˆ°: {cache_file}")
            print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(cache_file) / 1024 / 1024:.2f} MB")
            
        except Exception as e:
            print(f"âš ï¸ æ•°æ®ç¼“å­˜å¤±è´¥: {e}")
    
    def generate_coordinates(self, n_samples: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ç”Ÿæˆæ··åˆç­–ç•¥é‡‡æ ·åæ ‡"""
        print(f"âš–ï¸ ç”Ÿæˆæ··åˆç­–ç•¥åæ ‡: {n_samples} æ ·æœ¬")
        print("   ç­–ç•¥: 70%åŸå¸‚å¯†é›† + 30%å…¨è‹±éšæœº")
        
        np.random.seed(42)  # å›ºå®šç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
        
        # åˆ†é…æ¯”ä¾‹ï¼š70%åŸå¸‚å¯†é›†ï¼Œ30%å…¨è‹±éšæœº
        city_samples = int(n_samples * 0.7)
        random_samples = n_samples - city_samples
        
        latitudes = []
        longitudes = []
        months = []
        
        # åŸå¸‚å¯†é›†éƒ¨åˆ† (70%)
        samples_per_city = city_samples // 3
        remaining_city = city_samples % 3
        
        city_centers = {
            'London': {'lat': 51.5074, 'lon': -0.1278},
            'Manchester': {'lat': 53.4808, 'lon': -2.2426},
            'Edinburgh': {'lat': 55.9533, 'lon': -3.1883}
        }
        
        print(f"   ğŸ™ï¸ åŸå¸‚å¯†é›†é‡‡æ ·: {city_samples}ä¸ªæ ·æœ¬")
        for i, (city, center) in enumerate(city_centers.items()):
            city_sample_count = samples_per_city + (1 if i < remaining_city else 0)
            
            # åœ¨åŸå¸‚å‘¨å›´å¯†é›†é‡‡æ · (æ ‡å‡†å·®çº¦20-30km)
            city_lats = np.random.normal(center['lat'], 0.2, city_sample_count)
            city_lons = np.random.normal(center['lon'], 0.3, city_sample_count)
            city_months = np.random.randint(1, 13, city_sample_count)
            
            latitudes.extend(city_lats)
            longitudes.extend(city_lons)
            months.extend(city_months)
            
            print(f"     {city}: {city_sample_count}ä¸ªæ ·æœ¬ (Â±20-30kmèŒƒå›´)")
        
        # å…¨è‹±éšæœºéƒ¨åˆ† (30%)
        print(f"   ğŸŒ å…¨è‹±éšæœºé‡‡æ ·: {random_samples}ä¸ªæ ·æœ¬")
        random_lats = np.random.uniform(50.0, 60.0, random_samples)
        random_lons = np.random.uniform(-6.0, 2.0, random_samples)
        random_months = np.random.randint(1, 13, random_samples)
        
        latitudes.extend(random_lats)
        longitudes.extend(random_lons)
        months.extend(random_months)
        
        return np.array(latitudes), np.array(longitudes), np.array(months)
    
    def collect_training_data(self, n_samples: int = 50) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """æ”¶é›†è®­ç»ƒæ•°æ®ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰"""
        
        # æ£€æŸ¥ç¼“å­˜
        X_cached, y_climate_cached, y_geo_cached = self.load_cached_data(n_samples)
        if X_cached is not None:
            return X_cached, y_climate_cached, y_geo_cached
        
        # ç”Ÿæˆé‡‡æ ·åæ ‡
        latitudes, longitudes, months = self.generate_coordinates(n_samples)
        
        # åˆå§‹åŒ–æ•°æ®æ”¶é›†
        X = []
        y_climate = []
        y_geographic = []
        failed_samples = []
        
        print(f"\nğŸš€ å¼€å§‹æ”¶é›†{n_samples}ä¸ªæ ·æœ¬çš„è®­ç»ƒæ•°æ®...")
        print(f"ğŸ“¡ é¢„è®¡APIè°ƒç”¨æ¬¡æ•°: {n_samples * 5} (æ¯æ ·æœ¬5æ¬¡)")
        
        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
        with tqdm(total=n_samples, desc="ğŸŒ æ”¶é›†æ ·æœ¬", 
                 bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
            
            for i, (lat, lon, month) in enumerate(zip(latitudes, longitudes, months)):
                try:
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    pbar.set_description(f"ğŸŒ æ ·æœ¬ {i+1}/{n_samples} (APIè°ƒç”¨ä¸­...)")
                    
                    # ç”Ÿæˆ66ç‰¹å¾
                    features = self.feature_engineer.prepare_features_for_prediction(lat, lon, month, 66)
                    
                    # ç”Ÿæˆåˆæˆæ ‡ç­¾
                    climate_features = features[:44]  # å‰44ä¸ªæ˜¯æ»åç‰¹å¾
                    climate_score = np.tanh(np.mean(climate_features[:11]) / 100) * 2  # æ ‡å‡†åŒ–åˆ°[-2, 2]
                    
                    geo_features = features[44:]  # å22ä¸ªæ˜¯å˜åŒ–ç‡ç‰¹å¾
                    geographic_score = np.tanh(np.mean(geo_features) * 0.5) * 1.5  # æ ‡å‡†åŒ–åˆ°[-1.5, 1.5]
                    
                    X.append(features)
                    y_climate.append(climate_score)
                    y_geographic.append(geographic_score)
                    
                    # æ›´æ–°è¿›åº¦æ¡æè¿°ä¸ºæˆåŠŸ
                    pbar.set_description(f"âœ… æ ·æœ¬ {i+1}/{n_samples} (å®Œæˆ)")
                    
                except Exception as e:
                    failed_samples.append((i, lat, lon, month, str(e)))
                    pbar.set_description(f"âŒ æ ·æœ¬ {i+1}/{n_samples} (å¤±è´¥)")
                    tqdm.write(f"   âš ï¸ è·³è¿‡æ ·æœ¬ {i+1}: {e}")
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(1)
                
                # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡
                if (i + 1) % 10 == 0:
                    success_rate = (len(X) / (i + 1)) * 100
                    tqdm.write(f"   ğŸ“Š è¿›åº¦ç»Ÿè®¡: æˆåŠŸ {len(X)}/{i+1} ({success_rate:.1f}%)")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X = np.array(X) if X else np.array([]).reshape(0, 66)
        y_climate = np.array(y_climate) if y_climate else np.array([])
        y_geographic = np.array(y_geographic) if y_geographic else np.array([])
        
        # æ•°æ®æ”¶é›†æ€»ç»“
        print(f"\nâœ… æ•°æ®æ”¶é›†å®Œæˆ:")
        print(f"   ğŸ“Š æˆåŠŸæ ·æœ¬: {len(X)}/{n_samples} ({len(X)/n_samples*100:.1f}%)")
        print(f"   ğŸ“Š å¤±è´¥æ ·æœ¬: {len(failed_samples)}")
        print(f"   ğŸ“Š ç‰¹å¾å½¢çŠ¶: {X.shape}")
        
        if len(X) > 0:
            print(f"   ğŸ“Š Climateæ ‡ç­¾èŒƒå›´: [{y_climate.min():.3f}, {y_climate.max():.3f}]")
            print(f"   ğŸ“Š Geographicæ ‡ç­¾èŒƒå›´: [{y_geographic.min():.3f}, {y_geographic.max():.3f}]")
        
        if failed_samples:
            print(f"\nâš ï¸ å¤±è´¥æ ·æœ¬è¯¦æƒ…:")
            for idx, lat, lon, month, error in failed_samples[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   æ ·æœ¬{idx+1}: ({lat:.3f}, {lon:.3f}, æœˆä»½{month}) - {error}")
            if len(failed_samples) > 5:
                print(f"   ... è¿˜æœ‰ {len(failed_samples)-5} ä¸ªå¤±è´¥æ ·æœ¬")
        
        # ç«‹å³ä¿å­˜æ•°æ®åˆ°ç¼“å­˜
        if len(X) > 0:
            print(f"\nğŸ’¾ ç«‹å³ä¿å­˜æ•°æ®åˆ°ç¼“å­˜...")
            self.save_data_to_cache(X, y_climate, y_geographic, n_samples)
        else:
            print(f"\nâŒ æ²¡æœ‰æˆåŠŸæ”¶é›†åˆ°æ•°æ®ï¼Œè·³è¿‡ç¼“å­˜ä¿å­˜")
        
        return X, y_climate, y_geographic
    
    def train_single_model(self, model_type: str, X_train: np.ndarray, y_train: np.ndarray,
                          X_val: np.ndarray, y_val: np.ndarray, target_name: str,
                          fast_mode: bool = True) -> Tuple[Any, Dict]:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹ï¼ˆå¯é€‰å¿«é€Ÿæ¨¡å¼ï¼‰"""
        
        if model_type == 'RandomForest':
            return self._train_rf(X_train, y_train, target_name, fast_mode)
        elif model_type == 'LSTM':
            return self._train_lstm(X_train, y_train, X_val, y_val, target_name, fast_mode)
        elif model_type == '1D-CNN':
            return self._train_cnn1d(X_train, y_train, X_val, y_val, target_name, fast_mode)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
    
    def _train_rf(self, X_train: np.ndarray, y_train: np.ndarray, target_name: str, fast_mode: bool) -> Tuple[Any, Dict]:
        """è®­ç»ƒRandom Forestæ¨¡å‹"""
        print(f"ğŸŒ² è®­ç»ƒRandom Forestæ¨¡å‹ - {target_name}")
        
        if fast_mode:
            rf_config = {
                'n_estimators': 10,  # å¿«é€Ÿæ¨¡å¼ï¼šå‡å°‘æ ‘çš„æ•°é‡
                'max_depth': 5,      # å¿«é€Ÿæ¨¡å¼ï¼šå‡å°‘æ·±åº¦
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'random_state': 42,
                'n_jobs': -1
            }
            print("   ğŸš€ å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨è¾ƒå°çš„å‚æ•°")
        else:
            rf_config = {
                'n_estimators': 100,
                'max_depth': 20,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'random_state': 42,
                'n_jobs': -1
            }
        
        model = RandomForestRegressor(**rf_config)
        model.fit(X_train, y_train)
        
        # ç‰¹å¾é‡è¦æ€§
        feature_names = self.feature_engineer.get_feature_names()
        feature_importance = dict(zip(feature_names, model.feature_importances_))
        
        metadata = {
            'model_type': 'RandomForest',
            'config': rf_config,
            'feature_importance': feature_importance,
            'n_features': X_train.shape[1],
            'n_samples': X_train.shape[0],
            'fast_mode': fast_mode
        }
        
        print(f"   âœ… Random Forestè®­ç»ƒå®Œæˆ")
        return model, metadata
    
    def _train_lstm(self, X_train: np.ndarray, y_train: np.ndarray,
                   X_val: np.ndarray, y_val: np.ndarray, target_name: str, fast_mode: bool) -> Tuple[Any, Dict]:
        """è®­ç»ƒLSTMæ¨¡å‹"""
        print(f"ğŸ§  è®­ç»ƒLSTMæ¨¡å‹ - {target_name}")
        
        if not TF_AVAILABLE:
            raise RuntimeError("TensorFlowä¸å¯ç”¨ï¼Œæ— æ³•è®­ç»ƒLSTMæ¨¡å‹")
        
        # é‡å¡‘æ•°æ®
        X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
        X_val_lstm = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
        
        if fast_mode:
            # å¿«é€Ÿæ¨¡å¼ï¼šæ›´å°çš„ç½‘ç»œå’Œæ›´å°‘çš„è®­ç»ƒè½®æ•°
            model = Sequential([
                LSTM(16, return_sequences=False, input_shape=(66, 1)),
                Dense(8, activation='relu'),
                Dense(1, activation='linear')
            ])
            epochs = 5
            print("   ğŸš€ å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨è¾ƒå°çš„ç½‘ç»œå’Œæ›´å°‘è®­ç»ƒè½®æ•°")
        else:
            model = Sequential([
                LSTM(64, return_sequences=True, input_shape=(66, 1)),
                Dropout(0.2),
                LSTM(32, return_sequences=False),
                Dropout(0.2),
                Dense(16, activation='relu'),
                Dense(1, activation='linear')
            ])
            epochs = 50
        
        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
        
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=3 if fast_mode else 10, restore_best_weights=True)
        ]
        
        history = model.fit(
            X_train_lstm, y_train,
            validation_data=(X_val_lstm, y_val),
            epochs=epochs,
            batch_size=16 if fast_mode else 32,
            callbacks=callbacks,
            verbose=0
        )
        
        metadata = {
            'model_type': 'LSTM',
            'epochs_trained': len(history.history['loss']),
            'final_loss': float(history.history['loss'][-1]),
            'final_val_loss': float(history.history['val_loss'][-1]),
            'fast_mode': fast_mode
        }
        
        print(f"   âœ… LSTMè®­ç»ƒå®Œæˆï¼Œè®­ç»ƒäº†{metadata['epochs_trained']}è½®")
        return model, metadata
    
    def _train_cnn1d(self, X_train: np.ndarray, y_train: np.ndarray,
                    X_val: np.ndarray, y_val: np.ndarray, target_name: str, fast_mode: bool) -> Tuple[Any, Dict]:
        """è®­ç»ƒ1D CNNæ¨¡å‹"""
        print(f"ğŸ” è®­ç»ƒDeep 1D-CNNæ¨¡å‹ - {target_name}")
        
        if not TF_AVAILABLE:
            raise RuntimeError("TensorFlowä¸å¯ç”¨ï¼Œæ— æ³•è®­ç»ƒCNNæ¨¡å‹")
        
        # é‡å¡‘æ•°æ®
        X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
        X_val_cnn = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
        
        if fast_mode:
            # å¿«é€Ÿæ¨¡å¼ï¼šæ›´ç®€å•çš„ç½‘ç»œ
            model = Sequential([
                Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=(66, 1)),
                MaxPooling1D(pool_size=2),
                Conv1D(filters=8, kernel_size=3, activation='relu'),
                Flatten(),
                Dense(16, activation='relu'),
                Dense(1, activation='linear')
            ])
            epochs = 5
            print("   ğŸš€ å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨è¾ƒç®€å•çš„ç½‘ç»œ")
        else:
            model = Sequential([
                Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(66, 1)),
                MaxPooling1D(pool_size=2),
                Conv1D(filters=32, kernel_size=3, activation='relu'),
                MaxPooling1D(pool_size=2),
                Conv1D(filters=16, kernel_size=3, activation='relu'),
                Flatten(),
                Dense(50, activation='relu'),
                Dropout(0.3),
                Dense(25, activation='relu'),
                Dense(1, activation='linear')
            ])
            epochs = 50
        
        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
        
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=3 if fast_mode else 10, restore_best_weights=True)
        ]
        
        history = model.fit(
            X_train_cnn, y_train,
            validation_data=(X_val_cnn, y_val),
            epochs=epochs,
            batch_size=16 if fast_mode else 32,
            callbacks=callbacks,
            verbose=0
        )
        
        metadata = {
            'model_type': '1D-CNN',
            'epochs_trained': len(history.history['loss']),
            'final_loss': float(history.history['loss'][-1]),
            'final_val_loss': float(history.history['val_loss'][-1]),
            'fast_mode': fast_mode
        }
        
        print(f"   âœ… 1D-CNNè®­ç»ƒå®Œæˆï¼Œè®­ç»ƒäº†{metadata['epochs_trained']}è½®")
        return model, metadata
    
    def evaluate_model(self, model, X_test: np.ndarray, y_test: np.ndarray, model_type: str) -> Dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        
        # é¢„æµ‹
        if model_type in ['LSTM', '1D-CNN']:
            X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
            y_pred = model.predict(X_test_reshaped, verbose=0).flatten()
        else:  # RF
            y_pred = model.predict(X_test)
        
        # è®¡ç®—æŒ‡æ ‡
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        metrics = {
            'MSE': float(mse),
            'RMSE': float(rmse),
            'MAE': float(mae),
            'R2': float(r2)
        }
        
        print(f"     ğŸ“Š RMSE: {rmse:.4f}, RÂ²: {r2:.4f}, MAE: {mae:.4f}")
        return metrics

def main(n_samples: int = 20, fast_mode: bool = True):
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("ğŸš€ å¼€å§‹66ç‰¹å¾æ¨¡å‹è®­ç»ƒï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
    print("=" * 80)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ æ ·æœ¬æ•°é‡: {n_samples}")
    print(f"âš¡ å¿«é€Ÿæ¨¡å¼: {'å¼€å¯' if fast_mode else 'å…³é—­'}")
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = OptimizedModelTrainer()
    
    # é˜¶æ®µ1ï¼šæ•°æ®æ”¶é›†
    print(f"\n" + "=" * 60)
    print("ğŸ—‚ï¸ é˜¶æ®µ1: æ•°æ®æ”¶é›†")
    print("=" * 60)
    
    X, y_climate, y_geographic = trainer.collect_training_data(n_samples=n_samples)
    
    if len(X) == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸæ”¶é›†åˆ°æ•°æ®ï¼Œç»ˆæ­¢è®­ç»ƒ")
        return None
    
    # é˜¶æ®µ2ï¼šæ•°æ®åˆ†å‰²
    print(f"\n" + "=" * 60)
    print("ğŸ”„ é˜¶æ®µ2: æ•°æ®åˆ†å‰²")
    print("=" * 60)
    
    if len(X) < 10:
        print("âš ï¸ æ ·æœ¬æ•°é‡å¤ªå°‘ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²")
        # ç®€å•åˆ†å‰²
        split_idx = int(len(X) * 0.7)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_climate_train, y_climate_test = y_climate[:split_idx], y_climate[split_idx:]
        y_geo_train, y_geo_test = y_geographic[:split_idx], y_geographic[split_idx:]
        
        # éªŒè¯é›†å°±ç”¨è®­ç»ƒé›†
        X_val, y_climate_val, y_geo_val = X_train, y_climate_train, y_geo_train
    else:
        # æ­£å¸¸åˆ†å‰²
        X_train, X_test, y_climate_train, y_climate_test, y_geo_train, y_geo_test = train_test_split(
            X, y_climate, y_geographic, test_size=0.3, random_state=42)
        
        X_train, X_val, y_climate_train, y_climate_val, y_geo_train, y_geo_val = train_test_split(
            X_train, y_climate_train, y_geo_train, test_size=0.3, random_state=42)
    
    print(f"   è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # é˜¶æ®µ3ï¼šæ¨¡å‹è®­ç»ƒ
    print(f"\n" + "=" * 60)
    print("ğŸ¤– é˜¶æ®µ3: æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    # è¦è®­ç»ƒçš„æ¨¡å‹ç±»å‹
    model_types = ['RandomForest']
    if TF_AVAILABLE:
        model_types.extend(['LSTM', '1D-CNN'])
    
    # ç›®æ ‡å˜é‡
    targets = {
        'climate': (y_climate_train, y_climate_val, y_climate_test),
        'geographic': (y_geo_train, y_geo_val, y_geo_test)
    }
    
    results = {}
    
    for target_name, (y_train, y_val, y_test) in targets.items():
        print(f"\nğŸ¯ è®­ç»ƒ {target_name.upper()} æ¨¡å‹")
        results[target_name] = {}
        
        for model_type in model_types:
            print(f"\n   ğŸ”§ {model_type} - {target_name}")
            
            try:
                # é€‰æ‹©æ•°æ®
                if model_type == 'RandomForest':
                    X_train_use, X_val_use, X_test_use = X_train, X_val, X_test
                else:
                    X_train_use, X_val_use, X_test_use = X_train_scaled, X_val_scaled, X_test_scaled
                
                # è®­ç»ƒæ¨¡å‹
                model, metadata = trainer.train_single_model(
                    model_type, X_train_use, y_train, X_val_use, y_val, target_name, fast_mode
                )
                
                # è¯„ä¼°æ¨¡å‹
                metrics = trainer.evaluate_model(model, X_test_use, y_test, model_type)
                
                results[target_name][model_type] = {
                    'metrics': metrics,
                    'metadata': metadata
                }
                
                print(f"     âœ… {model_type} å®Œæˆ")
                
            except Exception as e:
                print(f"     âŒ {model_type} å¤±è´¥: {e}")
                results[target_name][model_type] = {'error': str(e)}
    
    # ç»“æœæ€»ç»“
    print(f"\n" + "=" * 80)
    print("ğŸ“Š è®­ç»ƒç»“æœæ€»ç»“")
    print("=" * 80)
    
    for target_name, target_results in results.items():
        print(f"\nğŸ¯ {target_name.upper()} æ¨¡å‹:")
        for model_type, result in target_results.items():
            if 'error' in result:
                print(f"   {model_type:>12}: âŒ {result['error']}")
            else:
                metrics = result['metrics']
                print(f"   {model_type:>12}: RÂ²={metrics['R2']:.3f}, RMSE={metrics['RMSE']:.3f}")
    
    print(f"\nâ° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ‰ å°è§„æ¨¡éªŒè¯å®Œæˆï¼")
    
    return results

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='66ç‰¹å¾æ¨¡å‹è®­ç»ƒ - ä¼˜åŒ–ç‰ˆæœ¬')
    parser.add_argument('--samples', type=int, default=20, help='æ ·æœ¬æ•°é‡ (é»˜è®¤: 20)')
    parser.add_argument('--full', action='store_true', help='å®Œæ•´æ¨¡å¼ï¼ˆå…³é—­å¿«é€Ÿæ¨¡å¼ï¼‰')
    
    args = parser.parse_args()
    
    # è¿è¡Œè®­ç»ƒ
    results = main(n_samples=args.samples, fast_mode=not args.full) 