#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Images API Routes - å›¾ç‰‡ä¸Šä¼ ä¸ç®¡ç†APIç«¯ç‚¹
"""

from flask import Blueprint, request, jsonify, current_app
import cloudinary.uploader
import psycopg2
import os
import logging
import hashlib
from datetime import datetime
from werkzeug.datastructures import FileStorage
import io
import json
import random
import hashlib
import uuid
import time

# SocketIOå¯¼å…¥
try:
    from flask_socketio import emit
    SOCKETIO_AVAILABLE = True
except ImportError:
    SOCKETIO_AVAILABLE = False

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    # ç¡®ä¿ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½.envæ–‡ä»¶
    # å½“å‰æ–‡ä»¶ï¼šapi/routes/images.pyï¼Œéœ€è¦å‘ä¸Šä¸¤çº§åˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•
    current_file = os.path.abspath(__file__)  # /path/to/project/api/routes/images.py
    routes_dir = os.path.dirname(current_file)  # /path/to/project/api/routes
    api_dir = os.path.dirname(routes_dir)  # /path/to/project/api
    project_root = os.path.dirname(api_dir)  # /path/to/project
    env_path = os.path.join(project_root, '.env')
    
    print(f"ğŸ” å½“å‰æ–‡ä»¶è·¯å¾„: {current_file}")
    print(f"ğŸ” è®¡ç®—çš„é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"ğŸ” .envæ–‡ä»¶è·¯å¾„: {env_path}")
    
    if os.path.exists(env_path):
        load_dotenv(env_path)
        print(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
    else:
        print(f"âš ï¸ ç¯å¢ƒå˜é‡æ–‡ä»¶ä¸å­˜åœ¨: {env_path}")
except ImportError:
    print("âš ï¸ python-dotenvæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½.envæ–‡ä»¶")

# éªŒè¯DeepSeek APIå¯†é’¥
deepseek_key = os.getenv('DEEPSEEK_API_KEY')
if deepseek_key:
    print(f"âœ… DeepSeek APIå¯†é’¥å·²åŠ è½½: {deepseek_key[:10]}...{deepseek_key[-5:]}")
else:
    print("âŒ DeepSeek APIå¯†é’¥æœªæ‰¾åˆ°")

# æ·»åŠ OpenAIå¯¼å…¥
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# æœ¬åœ°å›¾ç‰‡å­˜å‚¨ï¼ˆç”¨äºå¼€å‘ç¯å¢ƒï¼‰
LOCAL_IMAGES_STORE = {}

# æœ¬åœ°å¼€å‘æ—¶çš„åˆ†æç»“æœå­˜å‚¨
LOCAL_ANALYSIS_STORE = {}

logger = logging.getLogger(__name__)

# åˆ›å»ºè“å›¾
images_bp = Blueprint('images', __name__, url_prefix='/api/v1/images')

def emit_new_image_event(image_data):
    """å‘é€æ–°å›¾ç‰‡ä¸Šä¼ äº‹ä»¶åˆ°æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯"""
    try:
        if SOCKETIO_AVAILABLE and hasattr(current_app, 'socketio'):
            # ç›´æ¥ä½¿ç”¨SocketIOå®ä¾‹å‘é€äº‹ä»¶åˆ°æ‰€æœ‰å®¢æˆ·ç«¯
            socketio = current_app.socketio
            socketio.emit('new_image_uploaded', {
                'image_id': image_data.get('id'),
                'description': image_data.get('description', ''),
                'created_at': image_data.get('created_at'),
                'message': 'New environmental vision uploaded!'
            })
            logger.info(f"âœ… WebSocket event sent for image {image_data.get('id')}")
        else:
            logger.warning("âš ï¸ SocketIO not available, skipping event emission")
    except Exception as e:
        logger.error(f"âŒ Failed to emit WebSocket event: {e}")

def process_image_analysis(image_id, image_url, description, prediction_id):
    """
    åœ¨å›¾ç‰‡ä¸Šä¼ åç«‹å³è¿›è¡Œåˆ†æå’Œæ•…äº‹ç”Ÿæˆ
    
    Args:
        image_id: å›¾ç‰‡ID
        image_url: å›¾ç‰‡URL
        description: å›¾ç‰‡æè¿°
        prediction_id: é¢„æµ‹ID
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    try:
        logger.info(f"ğŸ”„ Starting analysis for image {image_id}")
        
        # 1. ç”ŸæˆSHAPåˆ†ææ•°æ®
        shap_data = generate_shap_analysis_data(image_id, description)
        
        # 2. ç”ŸæˆAIæ•…äº‹
        story_data = generate_ai_environmental_story(shap_data)
        
        # 3. ç»„åˆåˆ†æç»“æœ
        analysis_result = {
            'image_id': image_id,
            'shap_analysis': shap_data,
            'ai_story': story_data,
            'generated_at': datetime.now().isoformat(),
            'status': 'completed'
        }
        
        # 4. å­˜å‚¨åˆ†æç»“æœ
        try:
            # å°è¯•å­˜å‚¨åˆ°æ•°æ®åº“
            conn = psycopg2.connect(os.environ['DATABASE_URL'])
            cur = conn.cursor()
            
            cur.execute("""
                INSERT INTO image_analysis (image_id, shap_data, ai_story, generated_at) 
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (image_id) DO UPDATE SET
                    shap_data = EXCLUDED.shap_data,
                    ai_story = EXCLUDED.ai_story,
                    generated_at = EXCLUDED.generated_at
            """, (image_id, json.dumps(shap_data), json.dumps(story_data), datetime.now()))
            
            conn.commit()
            cur.close()
            conn.close()
            
            logger.info(f"âœ… Analysis stored in database for image {image_id}")
            
        except Exception as db_error:
            logger.error(f"Database storage failed: {db_error}")
            # å­˜å‚¨åˆ°æœ¬åœ°
            LOCAL_ANALYSIS_STORE[image_id] = analysis_result
            logger.info(f"âœ… Analysis stored locally for image {image_id}")
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"âŒ Error processing image analysis: {e}")
        return {
            'image_id': image_id,
            'error': str(e),
            'generated_at': datetime.now().isoformat(),
            'status': 'failed'
        }

def generate_shap_analysis_data(image_id, description):
    """
    ç”ŸæˆSHAPåˆ†ææ•°æ® - ä¿®å¤ç‰ˆï¼Œä½¿ç”¨çœŸå®æ•°æ®è€Œä¸æ˜¯ç¡¬ç¼–ç 
    
    Args:
        image_id: å›¾ç‰‡ID
        description: å›¾ç‰‡æè¿°
        
    Returns:
        dict: SHAPåˆ†ææ•°æ®
    """
    logger.info(f"ğŸ”„ Generating SHAP analysis for image {image_id} with description: {description[:50]}...")
    
    # ä½¿ç”¨æ–°çš„åŠ¨æ€åˆ†æå‡½æ•°
    try:
        dynamic_analysis = generate_dynamic_image_analysis(image_id)
        
        # æå–SHAPç›¸å…³æ•°æ®
        result_data = dynamic_analysis.get('result_data', {})
        
        # æ„å»ºä¸åŸæ ¼å¼å…¼å®¹çš„è¿”å›æ•°æ®
        shap_analysis_data = {
            'climate_score': result_data.get('climate_score', 0.5),
            'geographic_score': result_data.get('geographic_score', 0.5), 
            'economic_score': result_data.get('economic_score', 0.5),
            'final_score': result_data.get('final_score', 0.5),
            'city': result_data.get('city', 'Unknown Location'),
            'overall_confidence': result_data.get('confidence', 0.85),
            'shap_analysis': result_data.get('shap_analysis', {}),
            
            # ä¿ç•™åŸæœ‰å…¼å®¹å­—æ®µ
            'temperature': result_data.get('temperature', 20.0),
            'humidity': result_data.get('humidity', 60.0),
            'climate_type': result_data.get('climate_type', 'temperate'),
            'vegetation_index': result_data.get('vegetation_index', 0.7),
            'predictions': result_data.get('predictions', {
                'short_term': 'Moderate environmental conditions expected',
                'long_term': 'Stable climate trends anticipated'
            }),
            
            # æ•°æ®éªŒè¯ç»“æœ
            'is_valid': True,
            'validation_score': 0.94,
            'errors': [],
            'warnings': [],
            
            # åˆ†æå…ƒæ•°æ®
            'analysis_metadata': result_data.get('analysis_metadata', {
                'generated_at': datetime.now().isoformat(),
                'model_version': 'dynamic_shap_v1.0.0',
                'image_id': image_id,
                'description_based': True
            })
        }
        
        logger.info(f"âœ… Dynamic SHAP analysis completed for image {image_id}: final_score={shap_analysis_data['final_score']}")
        return shap_analysis_data
        
    except Exception as e:
        logger.error(f"âŒ Dynamic SHAP analysis failed for image {image_id}: {e}")
        
        # æœ€ç»ˆfallbackï¼šè¿”å›æœ€åŸºæœ¬çš„æ•°æ®ç»“æ„
        fallback_data = {
            'climate_score': 0.5,
            'geographic_score': 0.5,
            'economic_score': 0.5,
            'final_score': 0.5,
            'city': 'Unknown Location',
            'overall_confidence': 0.75,
            'shap_analysis': {
                'feature_importance': {
                    'temperature_trend': 0.15,
                    'humidity_factor': 0.12,
                    'geographic_position': 0.18
                }
            },
            'temperature': 20.0,
            'humidity': 60.0,
            'climate_type': 'temperate',
            'vegetation_index': 0.7,
            'predictions': {
                'short_term': 'Moderate environmental conditions expected',
                'long_term': 'Stable climate trends anticipated'
            },
            'is_valid': True,
            'validation_score': 0.75,
            'errors': [f'Dynamic analysis failed: {str(e)}'],
            'warnings': ['Using fallback SHAP data'],
            'analysis_metadata': {
                'generated_at': datetime.now().isoformat(),
                'model_version': 'fallback_v1.0.0',
                'image_id': image_id,
                'fallback_used': True
            }
        }
        
        logger.warning(f"âš ï¸ Using fallback SHAP data for image {image_id}")
        return fallback_data

def transform_to_hierarchical_shap_data(flat_shap_data):
    """
    å°†å¹³é¢çš„SHAPæ•°æ®è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„ï¼Œç”¨äºåœ†å½¢æ‰“åŒ…å›¾å¯è§†åŒ–
    
    Args:
        flat_shap_data: åŒ…å«flat feature_importanceçš„åŸå§‹SHAPæ•°æ®
        
    Returns:
        dict: å±‚æ¬¡åŒ–çš„SHAPæ•°æ®ç»“æ„
    """
    if not flat_shap_data or 'shap_analysis' not in flat_shap_data:
        return flat_shap_data
    
    original_features = flat_shap_data['shap_analysis'].get('feature_importance', {})
    
    # å®šä¹‰ç‰¹å¾åˆ°ç»´åº¦çš„æ˜ å°„
    feature_mapping = {
        'climate': {
            'temperature': original_features.get('temperature', 0.0),
            'humidity': original_features.get('humidity', 0.0),
            'climate_zone': original_features.get('climate_zone', 0.0),
            'precipitation': original_features.get('precipitation', 0.0),
            'wind_speed': original_features.get('wind_speed', 0.0)
        },
        'geographic': {
            'location_factor': original_features.get('location_factor', 0.0),
            'pressure': original_features.get('pressure', 0.0),
            'elevation': original_features.get('elevation', 0.0),
            'latitude': original_features.get('latitude', 0.0),
            'longitude': original_features.get('longitude', 0.0)
        },
        'economic': {
            'seasonal_factor': original_features.get('seasonal_factor', 0.0),
            'population_density': original_features.get('population_density', 0.0),
            'urban_index': original_features.get('urban_index', 0.0),
            'infrastructure': original_features.get('infrastructure', 0.0)
        }
    }
    
    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ€»é‡è¦æ€§
    dimension_scores = {}
    for dimension, features in feature_mapping.items():
        # è¿‡æ»¤æ‰å€¼ä¸º0çš„ç‰¹å¾
        active_features = {k: v for k, v in features.items() if v > 0}
        dimension_scores[dimension] = {
            'total_importance': sum(active_features.values()),
            'feature_count': len(active_features),
            'features': active_features
        }
    
    # åˆ›å»ºå¢å¼ºçš„SHAPæ•°æ®
    enhanced_shap_data = dict(flat_shap_data)
    enhanced_shap_data['shap_analysis']['hierarchical_features'] = {
        'climate': dimension_scores['climate'],
        'geographic': dimension_scores['geographic'], 
        'economic': dimension_scores['economic']
    }
    
    # æ·»åŠ åœ†å½¢æ‰“åŒ…å›¾æ‰€éœ€çš„æ•°æ®æ ¼å¼
    enhanced_shap_data['shap_analysis']['pack_chart_data'] = generate_pack_chart_data(
        dimension_scores, 
        flat_shap_data.get('final_score', 0.7)
    )
    
    return enhanced_shap_data

def generate_pack_chart_data(dimension_scores, final_score):
    """
    ç”Ÿæˆåœ†å½¢æ‰“åŒ…å›¾æ‰€éœ€çš„æ•°æ®æ ¼å¼
    
    Args:
        dimension_scores: ç»´åº¦å¾—åˆ†æ•°æ®
        final_score: æœ€ç»ˆå¾—åˆ†
        
    Returns:
        dict: åœ†å½¢æ‰“åŒ…å›¾æ•°æ®ç»“æ„
    """
    pack_data = {
        "name": "Environmental Impact",
        "value": final_score,
        "children": []
    }
    
    # ç»´åº¦é¢œè‰²æ˜ å°„ï¼ˆè’¸æ±½æœ‹å…‹ä¸»é¢˜ï¼‰
    dimension_colors = {
        'climate': '#d4af37',      # é‡‘è‰²
        'geographic': '#cd853f',    # ç§˜é²è‰²
        'economic': '#8b4513'       # é©¬éæ£•è‰²
    }
    
    for dimension, data in dimension_scores.items():
        if data['total_importance'] > 0:
            dimension_node = {
                "name": dimension.title(),
                "value": data['total_importance'],
                "itemStyle": {"color": dimension_colors.get(dimension, '#888888')},
                "children": []
            }
            
            # æ·»åŠ ç‰¹å¾èŠ‚ç‚¹
            for feature, importance in data['features'].items():
                if importance > 0:
                    feature_node = {
                        "name": feature.replace('_', ' ').title(),
                        "value": importance,
                        "itemStyle": {"color": dimension_colors.get(dimension, '#888888')},
                        "tooltip": {
                            "formatter": f"{feature.replace('_', ' ').title()}: {importance:.3f}"
                        }
                    }
                    dimension_node["children"].append(feature_node)
            
            pack_data["children"].append(dimension_node)
    
    return pack_data

def validate_hierarchical_shap_data(shap_data):
    """
    éªŒè¯å±‚æ¬¡åŒ–SHAPæ•°æ®çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§
    
    Args:
        shap_data: å±‚æ¬¡åŒ–çš„SHAPæ•°æ®
        
    Returns:
        dict: éªŒè¯ç»“æœï¼ŒåŒ…å«is_validå¸ƒå°”å€¼å’Œè¯¦ç»†æŠ¥å‘Š
    """
    validation_result = {
        "is_valid": True,
        "errors": [],
        "warnings": [],
        "completeness_report": {},
        "dimension_analysis": {}
    }
    
    # æ£€æŸ¥å¿…éœ€çš„é¡¶çº§å­—æ®µ
    required_fields = ['climate_score', 'geographic_score', 'economic_score', 'final_score']
    for field in required_fields:
        if field not in shap_data:
            validation_result["errors"].append(f"Missing required field: {field}")
            validation_result["is_valid"] = False
        elif not isinstance(shap_data[field], (int, float)):
            validation_result["errors"].append(f"Invalid type for {field}: expected number")
            validation_result["is_valid"] = False
    
    # æ£€æŸ¥SHAPåˆ†æç»“æ„
    if 'shap_analysis' not in shap_data:
        validation_result["errors"].append("Missing shap_analysis section")
        validation_result["is_valid"] = False
        return validation_result
    
    shap_section = shap_data['shap_analysis']
    
    # æ£€æŸ¥å±‚æ¬¡åŒ–ç‰¹å¾æ•°æ®
    if 'hierarchical_features' in shap_section:
        hierarchical = shap_section['hierarchical_features']
        required_dimensions = ['climate', 'geographic', 'economic']
        
        for dimension in required_dimensions:
            if dimension not in hierarchical:
                validation_result["errors"].append(f"Missing dimension: {dimension}")
                validation_result["is_valid"] = False
            else:
                dim_data = hierarchical[dimension]
                
                # éªŒè¯ç»´åº¦æ•°æ®ç»“æ„
                if 'total_importance' not in dim_data:
                    validation_result["warnings"].append(f"Missing total_importance for {dimension}")
                if 'features' not in dim_data:
                    validation_result["errors"].append(f"Missing features for {dimension}")
                    validation_result["is_valid"] = False
                else:
                    # ç»Ÿè®¡ç‰¹å¾æ•°é‡å’Œé‡è¦æ€§æ€»å’Œ
                    features = dim_data['features']
                    feature_count = len(features)
                    importance_sum = sum(features.values()) if features else 0
                    
                    validation_result["dimension_analysis"][dimension] = {
                        "feature_count": feature_count,
                        "importance_sum": importance_sum,
                        "features_list": list(features.keys())
                    }
                    
                    if feature_count == 0:
                        validation_result["warnings"].append(f"No active features for {dimension}")
    
    # æ£€æŸ¥åœ†å½¢æ‰“åŒ…å›¾æ•°æ®
    if 'pack_chart_data' in shap_section:
        pack_data = shap_section['pack_chart_data']
        if 'children' not in pack_data:
            validation_result["warnings"].append("Missing children in pack_chart_data")
        else:
            validation_result["completeness_report"]["pack_chart_children"] = len(pack_data['children'])
    
    # æ€»ä½“å®Œæ•´æ€§è¯„åˆ†
    total_features = sum(
        analysis.get('feature_count', 0) 
        for analysis in validation_result["dimension_analysis"].values()
    )
    validation_result["completeness_report"]["total_features"] = total_features
    validation_result["completeness_report"]["has_ai_story"] = 'ai_story' in shap_data
    
    return validation_result

def generate_ai_environmental_story(shap_data, force_unique=True):
    """
    ä½¿ç”¨DeepSeekç”Ÿæˆç¯å¢ƒæ•…äº‹ï¼ˆçº¦100è¯è‹±æ–‡ï¼Œæˆå‰§æ€§æè¿°ï¼‰
    
    Args:
        shap_data: SHAPåˆ†ææ•°æ®ï¼ŒåŒ…å«ä¸‰ä¸ªç»´åº¦å¾—åˆ†å’Œç‰¹å¾é‡è¦æ€§
        force_unique: æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆå”¯ä¸€æ•…äº‹ï¼ˆé»˜è®¤Trueï¼‰
        
    Returns:
        str: ç”Ÿæˆçš„è‹±æ–‡ç¯å¢ƒæ•…äº‹
    """
    # è·å–DeepSeek APIå¯†é’¥
    deepseek_key = os.getenv('DEEPSEEK_API_KEY')
    if not deepseek_key:
        logger.warning("DeepSeek API key not found, using fallback story")
        return generate_fallback_story(shap_data)
    
    try:
        import requests
        import random
        import uuid
        
        # æ„å»ºç”¨äºæ•…äº‹ç”Ÿæˆçš„prompt
        climate_score = shap_data.get('climate_score', 0.5) * 100
        geographic_score = shap_data.get('geographic_score', 0.5) * 100  
        economic_score = shap_data.get('economic_score', 0.5) * 100
        city = shap_data.get('city', 'Unknown Location')
        
        # è·å–ä¸»è¦ç‰¹å¾å½±å“
        feature_importance = shap_data.get('shap_analysis', {}).get('feature_importance', {})
        top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # ğŸ”§ ä¿®å¤ï¼šå¢å¼ºéšæœºæ€§ï¼Œç¡®ä¿æ¯æ¬¡åˆ·æ–°éƒ½ç”Ÿæˆå®Œå…¨ä¸åŒçš„æ•…äº‹
        import time
        import os
        
        # ä½¿ç”¨å¤šé‡éšæœºæºç¡®ä¿å”¯ä¸€æ€§
        current_time = time.time()
        microseconds = int((current_time * 1000000) % 1000000)  # å¾®ç§’çº§æ—¶é—´æˆ³
        process_id = os.getpid() % 10000  # è¿›ç¨‹ID
        random_uuid = str(uuid.uuid4())[:8]  # éšæœºUUIDç‰‡æ®µ
        random_number = random.randint(100000, 999999)  # çº¯éšæœºæ•°
        
        # åˆ›å»ºå¼ºéšæœºæ€§æ ‡è¯†ç¬¦
        unique_id = f"{microseconds}_{process_id}_{random_uuid}_{random_number}"
        image_hash = hashlib.md5(f"{city}_{climate_score}_{geographic_score}_{economic_score}_{unique_id}".encode()).hexdigest()[:8]
        
        # å¤§å¹…æ‰©å±•æ•…äº‹é£æ ¼é€‰é¡¹å’Œéšæœºå…ƒç´ 
        story_styles = [
            "like a scene from a climate science thriller",
            "as if narrated by a future environmental historian", 
            "in the style of a dramatic weather report from 2050",
            "like an excerpt from an environmental documentary",
            "as a dramatic eyewitness account from the future",
            "in the tone of a scientific expedition journal",
            "like a chapter from a climate change novel",
            "as told by a time traveler from 2080",
            "in the voice of an AI environmental analyst",
            "like a dramatic news report from the future",
            "as a poetic environmental meditation",
            "in the style of a survival story",
            "like a letter from a climate refugee",
            "as an urgent environmental briefing",
            "in the tone of a nature documentary narrator"
        ]
        
        # æ·»åŠ éšæœºæƒ…æ„ŸåŸºè°ƒ
        emotional_tones = [
            "with urgent concern and hope",
            "with dramatic tension and mystery",
            "with scientific wonder and awe",
            "with melancholic beauty",
            "with fierce determination",
            "with quiet contemplation",
            "with explosive energy",
            "with gentle optimism",
            "with stark realism",
            "with poetic elegance"
        ]
        
        # æ·»åŠ éšæœºè§†è§’
        perspectives = [
            "from the perspective of the environment itself",
            "through the eyes of a scientist",
            "from a bird's eye view",
            "from ground level",
            "through the lens of time",
            "from multiple viewpoints",
            "through natural elements",
            "from an urban perspective",
            "through seasonal changes",
            "from a global viewpoint"
        ]
        
        # ä½¿ç”¨çœŸæ­£çš„éšæœºç§å­ï¼ˆä¸åŸºäºimage_idï¼‰
        random.seed(int(current_time * 1000000) % 2**32)
        
        # éšæœºé€‰æ‹©é£æ ¼å…ƒç´ 
        style_hint = random.choice(story_styles)
        emotional_tone = random.choice(emotional_tones)
        perspective = random.choice(perspectives)
        
        # æ·»åŠ éšæœºçš„ç‰¹æ®ŠæŒ‡ä»¤
        special_instructions = [
            "Include metaphors from nature.",
            "Use contrasting imagery.",
            "Focus on the human element.",
            "Emphasize the passage of time.",
            "Include sensory details.",
            "Use symbolism.",
            "Create dramatic tension.",
            "Include environmental sounds.",
            "Use color imagery.",
            "Focus on transformation."
        ]
        special_instruction = random.choice(special_instructions)
        
        # æ„å»ºé«˜åº¦éšæœºåŒ–çš„prompt
        prompt = f"""Write a dramatic environmental narrative in exactly 100 words for Analysis #{image_hash}. 

Location: {city}
Climate Impact: {climate_score:.1f}%
Geographic Impact: {geographic_score:.1f}% 
Economic Impact: {economic_score:.1f}%
Key factors: {', '.join([f[0] for f in top_features[:3]])}
Unique Session: {unique_id}

Create a COMPLETELY UNIQUE compelling story that dramatically describes the environmental conditions and future predictions for this specific location and data combination. Write {style_hint}, {emotional_tone}, and {perspective}.

Special instruction: {special_instruction}

CRITICAL: This story MUST be entirely different from any previous analysis. Use completely different narrative elements, vocabulary, metaphors, and dramatic structures. Each story should feel like it was written by a different author with a unique style.

Write EXACTLY 100 words. Be dramatic, engaging, and absolutely unique."""

        # è°ƒç”¨DeepSeek API with higher temperature for more creativity
        headers = {
            'Authorization': f'Bearer {deepseek_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": "You are a creative environmental storyteller who writes dramatically different narratives each time. Never repeat styles, themes, or approaches. Be completely unique and original in every story."},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 150,
            "temperature": 0.95,  # å¢åŠ æ¸©åº¦ä»¥è·å¾—æ›´å¤šåˆ›é€ æ€§
            "top_p": 0.9,        # æ·»åŠ nucleus sampling
            "frequency_penalty": 0.5,  # å‡å°‘é‡å¤
            "presence_penalty": 0.5    # é¼“åŠ±æ–°é¢–æ€§
        }
        
        response = requests.post(
            'https://api.deepseek.com/v1/chat/completions',
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            story = result['choices'][0]['message']['content'].strip()
            logger.info(f"âœ… DeepSeek AI story generated successfully for {city} (ID: {unique_id[:8]})")
            return story
        else:
            logger.error(f"âŒ DeepSeek API error: {response.status_code} - {response.text}")
            return generate_fallback_story(shap_data)
        
    except Exception as e:
        logger.error(f"âŒ DeepSeek story generation failed: {e}")
        return generate_fallback_story(shap_data)

def generate_fallback_story(shap_data):
    """
    ç”Ÿæˆå¤‡ç”¨æ•…äº‹ï¼ˆå½“OpenAIä¸å¯ç”¨æ—¶ï¼‰
    """
    climate_score = shap_data.get('climate_score', 0.5) * 100
    geographic_score = shap_data.get('geographic_score', 0.5) * 100
    economic_score = shap_data.get('economic_score', 0.5) * 100
    city = shap_data.get('city', 'Unknown Location')
    
    # åŸºäºå¾—åˆ†ç”Ÿæˆä¸åŒçš„æ•…äº‹æ¨¡æ¿
    if climate_score > 70:
        climate_desc = "thriving under stable atmospheric conditions"
    elif climate_score > 50:
        climate_desc = "adapting to moderate environmental pressures"
    else:
        climate_desc = "struggling against challenging climatic forces"
        
    if geographic_score > 70:
        geo_desc = "blessed with favorable topographical features"
    elif geographic_score > 50:
        geo_desc = "shaped by diverse geographical influences"
    else:
        geo_desc = "constrained by complex terrain challenges"
        
    if economic_score > 70:
        econ_desc = "supported by robust economic foundations"
    elif economic_score > 50:
        econ_desc = "balanced between growth and sustainability"
    else:
        econ_desc = "facing economic transformation pressures"
    
    story = f"""In {city}, an intricate environmental drama unfolds. The ecosystem stands {climate_desc}, while being {geo_desc}. The region remains {econ_desc}. Through SHAP analysis, we witness nature's delicate balance - where climate forces ({climate_score:.1f}%), geographic patterns ({geographic_score:.1f}%), and economic dynamics ({economic_score:.1f}%) converge to shape tomorrow's environmental narrative. This location tells a story of resilience, adaptation, and the profound interconnectedness of our planet's complex systems."""
    
    logger.info(f"âœ… Fallback story generated for {city}")
    return story

@images_bp.route('', methods=['POST'])
def upload_image():
    """
    å›¾ç‰‡ä¸Šä¼ APIç«¯ç‚¹
    
    æ¥æ”¶: multipart/form-data åŒ…å« file, description, prediction_id
    è¿”å›: å›¾ç‰‡ä¿¡æ¯JSON
    """
    try:
        # æ·»åŠ è°ƒè¯•ä»£ç  - å¼€å§‹
        logger.info("=== DEBUG: Upload request received ===")
        logger.info(f"Request content type: {request.content_type}")
        logger.info(f"Request files keys: {list(request.files.keys())}")
        logger.info(f"Request form keys: {list(request.form.keys())}")
        
        # æ£€æŸ¥åŸå§‹æ•°æ®
        logger.info(f"Request data length: {len(request.data)}")
        logger.info(f"Request stream available: {hasattr(request, 'stream')}")
        
        # å°è¯•æ‰‹åŠ¨è§£ææ–‡ä»¶ - å¦‚æœFlaskè§£æå¤±è´¥
        file = None
        description = request.form.get('description', '')
        prediction_id = request.form.get('prediction_id')
        
        # æ–¹æ³•1: æ ‡å‡†Flaskæ–¹å¼
        if 'file' in request.files:
            file = request.files['file']
            logger.info(f"Found file via Flask: {file.filename}")
        
        # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦æœ‰Noneé”®ï¼ˆè¯´æ˜è§£æå‡ºé”™ï¼‰
        elif None in request.form:
            # æ–‡ä»¶å†…å®¹è¢«å½“ä½œè¡¨å•æ•°æ®äº†ï¼Œå°è¯•é‡å»ºFileStorageå¯¹è±¡
            file_content = request.form[None]
            if file_content and file_content.startswith(b'\xff\xd8\xff') or file_content.startswith('JFIF'):
                # è¿™æ˜¯JPEGæ–‡ä»¶å†…å®¹
                file_bytes = file_content.encode('latin-1') if isinstance(file_content, str) else file_content
                file = FileStorage(
                    stream=io.BytesIO(file_bytes),
                    filename="uploaded_image.jpg",
                    content_type="image/jpeg"
                )
                logger.info("Reconstructed file from form data")
        
        # è°ƒè¯•ä»£ç  - ç»“æŸ
        logger.info(f"Final extracted - file: {file}, description: {description}, prediction_id: {prediction_id}")
        
        if not file or file.filename == '':
            return jsonify({
                "success": False,
                "error": "No file provided",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        # éªŒè¯æ–‡ä»¶ç±»å‹
        allowed_extensions = {'png', 'jpg', 'jpeg', 'gif', 'webp'}
        file_ext = file.filename.rsplit('.', 1)[1].lower() if '.' in file.filename else ''
        
        if file_ext not in allowed_extensions:
            return jsonify({
                "success": False,
                "error": f"Invalid file type. Allowed: {', '.join(allowed_extensions)}",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        # ä¸Šä¼ å›¾ç‰‡åˆ°Cloudinary
        try:
            upload_result = cloudinary.uploader.upload(
                file,
                folder="obscura_images",
                use_filename=True,
                unique_filename=True
            )
            
            image_url = upload_result['secure_url']
            thumbnail_url = upload_result.get('secure_url', image_url)
            
            logger.info(f"Image uploaded to Cloudinary: {image_url}")
            
        except Exception as e:
            logger.error(f"Cloudinary upload failed: {e}")
            return jsonify({
                "success": False,
                "error": f"Image upload failed: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }), 500
        
        # æœ¬åœ°å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨æœ¬åœ°å­˜å‚¨ä»£æ›¿æ•°æ®åº“
        database_url = os.getenv("DATABASE_URL")
        if not database_url or "nodename nor servname provided" in str(database_url):
            logger.info("Using local storage for development environment")
            
            # ç”Ÿæˆæœ¬åœ°å›¾ç‰‡ID
            image_id = len(LOCAL_IMAGES_STORE) + 1
            created_at = datetime.now()
            
            # å­˜å‚¨åˆ°æœ¬åœ°
            LOCAL_IMAGES_STORE[image_id] = {
                'id': image_id,
                'url': image_url,
                'thumbnail_url': thumbnail_url,
                'description': description,
                'prediction_id': int(prediction_id) if prediction_id else 1,
                'created_at': created_at
            }
            
            logger.info(f"Image stored locally with ID: {image_id}")
            
            # æ„å»ºå›¾ç‰‡æ•°æ®ç”¨äºWebSocketäº‹ä»¶
            image_data = {
                "id": image_id,
                "url": image_url,
                "thumbnail_url": thumbnail_url,
                "description": description,
                "prediction_id": int(prediction_id) if prediction_id else 1,
                "created_at": created_at.isoformat()
            }
            
            # å‘é€WebSocketäº‹ä»¶
            emit_new_image_event(image_data)
            
            return jsonify({
                "success": True,
                "image": image_data,
                "message": "Image uploaded successfully (local dev mode)",
                "timestamp": datetime.now().isoformat()
            }), 201
        
        # ä¿å­˜å›¾ç‰‡ä¿¡æ¯åˆ°æ•°æ®åº“ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
        try:
            conn = psycopg2.connect(os.environ['DATABASE_URL'])
            cur = conn.cursor()
            
            # ä¸ºæ¯ä¸ªå›¾ç‰‡åˆ›å»ºæ–°çš„predictionè®°å½•
            # è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„prediction ID
            cur.execute("SELECT COALESCE(MAX(id), 0) + 1 FROM predictions")
            prediction_id_int = cur.fetchone()[0]
            
            logger.info(f"Creating new prediction record with ID: {prediction_id_int}")
            
            # ä½¿ç”¨é»˜è®¤ç¯å¢ƒæ•°æ®
            environmental_data = {
                'latitude': 51.5074,
                'longitude': -0.1278,
                'temperature': 15.0,
                'humidity': 60.0,
                'pressure': 1013.0,
                'wind_speed': 0.0,
                'weather_description': 'clear',
                'timestamp': datetime.now().isoformat(),
                'month': datetime.now().month,
                'future_years': 0
            }
            
            # åˆ›å»ºç®€å•çš„fallback result_data
            result_data = _create_fallback_result_data(environmental_data)
            
            # åˆ›å»ºpredictionè®°å½•
            cur.execute("""
                INSERT INTO predictions (
                    id, input_data, result_data, prompt, location, created_at
                ) VALUES (
                    %s, %s, %s, %s, %s, %s
                ) ON CONFLICT (id) DO NOTHING
            """, (
                prediction_id_int,
                json.dumps(environmental_data),
                json.dumps(result_data),
                'SHAP-based environmental analysis for telescope image',
                'Unknown Location',
                datetime.now()
            ))
            logger.info(f"âœ… Prediction record created with ID: {prediction_id_int}")
            
            # ç°åœ¨æ’å…¥imageè®°å½•
            cur.execute("""
                INSERT INTO images (url, thumbnail_url, description, prediction_id, created_at) 
                VALUES (%s, %s, %s, %s, %s) 
                RETURNING id, created_at
            """, (image_url, thumbnail_url, description, prediction_id_int, datetime.now()))
            
            result = cur.fetchone()
            image_id, created_at = result
            
            conn.commit()
            cur.close()
            conn.close()
            
            logger.info(f"Image record saved to database with ID: {image_id}")
            
        except Exception as e:
            logger.error(f"Database insert failed: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“è¿æ¥é—®é¢˜æˆ–æ¶æ„é—®é¢˜
            database_issues = [
                "nodename nor servname provided",
                "could not translate host name", 
                "column \"temperature\" of relation \"predictions\" does not exist",
                "relation \"predictions\" does not exist",
                "does not exist"
            ]
            
            is_database_issue = any(issue in str(e) for issue in database_issues)
            
            if is_database_issue:
                logger.info(f"Database issue detected - using local storage mode")
                
                # ç”Ÿæˆæœ¬åœ°ID
                new_image_id = max(LOCAL_IMAGES_STORE.keys()) + 1 if LOCAL_IMAGES_STORE else 1
                
                # åˆ›å»ºæœ¬åœ°è®°å½•
                LOCAL_IMAGES_STORE[new_image_id] = {
                    'id': new_image_id,
                    'url': image_url,
                    'thumbnail_url': thumbnail_url,
                    'description': description,
                    'prediction_id': int(prediction_id),
                    'created_at': datetime.now(),
                    'location': 'Fallback Local Storage'
                }
                
                logger.info(f"Image stored locally with ID: {new_image_id}")
                
                # å¯åŠ¨åå°åˆ†æä»»åŠ¡
                import threading
                
                def run_analysis():
                    try:
                        # è¿è¡Œåˆ†æ
                        result = process_image_analysis(new_image_id, image_url, description, int(prediction_id))
                        logger.info(f"ğŸ“Š Analysis completed for image {new_image_id}: {result['status']}")
                    except Exception as e:
                        logger.error(f"âŒ Background analysis failed: {e}")
                
                # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œåˆ†æ
                analysis_thread = threading.Thread(target=run_analysis)
                analysis_thread.daemon = True
                analysis_thread.start()
                
                # æ„å»ºå›¾ç‰‡æ•°æ®ç”¨äºWebSocketäº‹ä»¶
                image_data_fallback = {
                    "id": new_image_id,
                    "url": image_url,
                    "thumbnail_url": thumbnail_url,
                    "description": description,
                    "prediction_id": int(prediction_id),
                    "created_at": datetime.now().isoformat()
                }
                
                # å‘é€WebSocketäº‹ä»¶
                emit_new_image_event(image_data_fallback)
                
                return jsonify({
                    "success": True,
                    "image": image_data_fallback,
                    "message": "Image uploaded successfully (fallback local storage mode)",
                    "analysis_status": "processing",
                    "timestamp": datetime.now().isoformat()
                }), 201
            else:
                return jsonify({
                    "success": False,
                    "error": f"Database save failed: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }), 500
        
        # å¯åŠ¨åå°åˆ†æä»»åŠ¡ï¼ˆæ•°æ®åº“æ¨¡å¼ï¼‰
        import threading
        
        def run_analysis():
            try:
                # è¿è¡Œåˆ†æ
                result = process_image_analysis(image_id, image_url, description, int(prediction_id))
                logger.info(f"ğŸ“Š Analysis completed for image {image_id}: {result['status']}")
            except Exception as e:
                logger.error(f"âŒ Background analysis failed: {e}")
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œåˆ†æ
        analysis_thread = threading.Thread(target=run_analysis)
        analysis_thread.daemon = True
        analysis_thread.start()
        
        # æ„å»ºå›¾ç‰‡æ•°æ®ç”¨äºWebSocketäº‹ä»¶
        image_data_main = {
            "id": image_id,
            "url": image_url,
            "thumbnail_url": thumbnail_url,
            "description": description,
            "prediction_id": int(prediction_id),
            "created_at": created_at.isoformat()
        }
        
        # å‘é€WebSocketäº‹ä»¶
        emit_new_image_event(image_data_main)
        
        # è¿”å›æˆåŠŸå“åº”
        return jsonify({
            "success": True,
            "image": image_data_main,
            "message": "Image uploaded successfully",
            "analysis_status": "processing",
            "timestamp": datetime.now().isoformat()
        }), 201
        
    except Exception as e:
        logger.error(f"Unexpected error in upload_image: {e}")
        return jsonify({
            "success": False,
            "error": "Internal server error",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/register', methods=['POST'])
def register_image():
    """
    å›¾ç‰‡URLæ³¨å†ŒAPIç«¯ç‚¹ - ä¸“é—¨ç”¨äºæ³¨å†Œå·²ä¸Šä¼ åˆ°Cloudinaryçš„å›¾ç‰‡
    
    æ¥æ”¶: JSONæ ¼å¼çš„å›¾ç‰‡URLå’Œå…ƒæ•°æ®
    è¿”å›: å›¾ç‰‡ä¿¡æ¯JSON
    """
    try:
        # éªŒè¯è¯·æ±‚æ ¼å¼
        if not request.is_json:
            return jsonify({
                "success": False,
                "error": "Request must be JSON format",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        data = request.get_json()
        if not data:
            return jsonify({
                "success": False,
                "error": "No JSON data provided",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        # éªŒè¯å¿…è¦å‚æ•°
        if 'url' not in data:
            return jsonify({
                "success": False,
                "error": "Missing required parameter: url",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        # æå–å‚æ•°
        image_url = data['url']
        description = data.get('description', 'Telescope generated artwork')
        source = data.get('source', 'cloudinary_telescope')
        metadata = data.get('metadata', {})
        
        # é»˜è®¤prediction_id - å¯ä»¥ä»metadataä¸­æå–æˆ–ä½¿ç”¨é»˜è®¤å€¼
        prediction_id = 1  # é»˜è®¤é¢„æµ‹ID
        if metadata and 'style' in metadata and 'prediction_id' in metadata['style']:
            try:
                prediction_id = int(metadata['style']['prediction_id'])
            except (ValueError, TypeError):
                prediction_id = 1
        
        # ç”Ÿæˆç¼©ç•¥å›¾URLï¼ˆCloudinaryè‡ªåŠ¨ç”Ÿæˆï¼‰
        thumbnail_url = image_url
        
        logger.info(f"Registering Cloudinary image: {image_url}")
        
        # ä¿å­˜å›¾ç‰‡ä¿¡æ¯åˆ°æ•°æ®åº“
        try:
            conn = psycopg2.connect(os.environ['DATABASE_URL'])
            cur = conn.cursor()
            
            # è‡ªåŠ¨åˆ›å»ºpredictionè®°å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            # æ£€æŸ¥predictionè®°å½•æ˜¯å¦å­˜åœ¨
            cur.execute("SELECT id FROM predictions WHERE id = %s", (prediction_id,))
            existing_prediction = cur.fetchone()
            
            if not existing_prediction:
                logger.info(f"Creating missing prediction record with ID: {prediction_id}")
                
                # åˆ›å»ºé»˜è®¤çš„predictionè®°å½•
                cur.execute("""
                    INSERT INTO predictions (
                        id, input_data, result_data, prompt, location, created_at
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s
                    ) ON CONFLICT (id) DO NOTHING
                """, (
                    prediction_id,
                    json.dumps({
                        "temperature": 15.0,
                        "humidity": 60.0,
                        "location": "London, UK",
                        "timestamp": datetime.now().isoformat()
                    }),
                    json.dumps({
                        "temperature": 15.0,
                        "humidity": 60.0,
                        "confidence": 1.0,
                        "climate_type": "temperate",
                        "vegetation_index": 0.75,
                        "predictions": {
                            "short_term": "System generated placeholder",
                            "long_term": "Stable conditions expected"
                        }
                    }),
                    'System generated prediction for image registration',
                    'London, UK',
                    datetime.now()
                ))
                logger.info(f"âœ… Default prediction record created with ID: {prediction_id}")
            
            # ç°åœ¨æ’å…¥imageè®°å½•
            cur.execute("""
                INSERT INTO images (url, thumbnail_url, description, prediction_id, created_at) 
                VALUES (%s, %s, %s, %s, %s) 
                RETURNING id, created_at
            """, (image_url, thumbnail_url, description, prediction_id, datetime.now()))
            
            result = cur.fetchone()
            image_id, created_at = result
            
            conn.commit()
            cur.close()
            conn.close()
            
            logger.info(f"Image registered in database with ID: {image_id}")
            
        except Exception as e:
            logger.error(f"Database insert failed: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“è¿æ¥é—®é¢˜æˆ–æ¶æ„é—®é¢˜
            database_issues = [
                "nodename nor servname provided",
                "could not translate host name",
                "column \"temperature\" of relation \"predictions\" does not exist", 
                "relation \"predictions\" does not exist",
                "does not exist"
            ]
            
            is_database_issue = any(issue in str(e) for issue in database_issues)
            
            if is_database_issue:
                logger.info(f"Database issue detected - using local storage mode for registration")
                
                # ç”Ÿæˆæœ¬åœ°ID  
                new_image_id = max(LOCAL_IMAGES_STORE.keys()) + 1 if LOCAL_IMAGES_STORE else 1
                created_at = datetime.now()
                
                # åˆ›å»ºæœ¬åœ°è®°å½•
                LOCAL_IMAGES_STORE[new_image_id] = {
                    'id': new_image_id,
                    'url': image_url,
                    'thumbnail_url': thumbnail_url,
                    'description': description,
                    'prediction_id': prediction_id,
                    'created_at': created_at,
                    'source': source,
                    'location': 'Fallback Local Registration'
                }
                
                logger.info(f"Image registered locally with ID: {new_image_id}")
                
                # æ„å»ºå›¾ç‰‡æ•°æ®ç”¨äºWebSocketäº‹ä»¶
                image_data_register_fallback = {
                    "id": new_image_id,
                    "url": image_url,
                    "thumbnail_url": thumbnail_url,
                    "description": description,
                    "prediction_id": prediction_id,
                    "created_at": created_at.isoformat(),
                    "source": source
                }
                
                # å‘é€WebSocketäº‹ä»¶
                emit_new_image_event(image_data_register_fallback)
                
                return jsonify({
                    "success": True,
                    "image": image_data_register_fallback,
                    "message": "Image registered successfully (fallback local storage mode)",
                    "timestamp": datetime.now().isoformat()
                }), 201
            else:
                return jsonify({
                    "success": False,
                    "error": f"Database save failed: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }), 500
        
        # æ„å»ºå›¾ç‰‡æ•°æ®ç”¨äºWebSocketäº‹ä»¶
        image_data_register_main = {
            "id": image_id,
            "url": image_url,
            "thumbnail_url": thumbnail_url,
            "description": description,
            "prediction_id": prediction_id,
            "created_at": created_at.isoformat(),
            "source": source
        }
        
        # å‘é€WebSocketäº‹ä»¶
        emit_new_image_event(image_data_register_main)
        
        # è¿”å›æˆåŠŸå“åº”
        return jsonify({
            "success": True,
            "image": image_data_register_main,
            "message": "Image registered successfully",
            "timestamp": datetime.now().isoformat()
        }), 201
        
    except Exception as e:
        logger.error(f"Unexpected error in register_image: {e}")
        return jsonify({
            "success": False,
            "error": "Internal server error",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('', methods=['GET'])
def get_images():
    """
    è·å–å›¾ç‰‡åˆ—è¡¨APIç«¯ç‚¹
    
    è¿”å›: æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯çš„JSONåˆ—è¡¨ï¼ŒåŒ…å«åŸºæœ¬é¢„æµ‹ä¿¡æ¯
    """
    images = []
    
    # é¦–å…ˆå°è¯•ä»æ•°æ®åº“è·å–å›¾ç‰‡
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # è”æŸ¥å›¾ç‰‡å’Œé¢„æµ‹æ•°æ®ï¼Œè·å–åŸºæœ¬ä¿¡æ¯
        cur.execute("""
            SELECT 
                i.id, i.url, i.thumbnail_url, i.description, i.prediction_id, i.created_at,
                p.location, p.input_data, p.result_data
            FROM images i
            LEFT JOIN predictions p ON i.prediction_id = p.id
            ORDER BY i.created_at DESC
        """)
        
        for row in cur.fetchall():
            image_data = {
                "id": row[0],
                "url": row[1],
                "thumbnail_url": row[2],
                "description": row[3],
                "prediction_id": row[4],
                "created_at": row[5].isoformat()
            }
            
            # æ·»åŠ åŸºæœ¬é¢„æµ‹ä¿¡æ¯
            if row[6] or row[7] or row[8]:  # å¦‚æœæœ‰é¢„æµ‹æ•°æ®
                prediction_info = {
                    "location": row[6],
                    "input_data": row[7] or {},
                    "result_data": row[8] or {}
                }
                image_data["prediction"] = prediction_info
            
            images.append(image_data)
        
        cur.close()
        conn.close()
        
        logger.info(f"Retrieved {len(images)} images from database with prediction data")
        
    except Exception as e:
        logger.error(f"Error fetching images from database: {e}")
        # æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°å­˜å‚¨
        
    # å¦‚æœæ•°æ®åº“ä¸ºç©ºæˆ–æŸ¥è¯¢å¤±è´¥ï¼Œæ£€æŸ¥æœ¬åœ°å­˜å‚¨
    if not images and LOCAL_IMAGES_STORE:
        logger.info("Database returned no images, checking local storage")
        
        # è½¬æ¢æœ¬åœ°å­˜å‚¨æ ¼å¼ï¼Œæ·»åŠ mock predictionæ•°æ®
        for image_id, image_data in LOCAL_IMAGES_STORE.items():
            image_info = {
                "id": image_data['id'],
                "url": image_data['url'],
                "thumbnail_url": image_data.get('thumbnail_url', image_data['url']),
                "description": image_data['description'],
                "prediction_id": image_data['prediction_id'],
                "created_at": image_data['created_at'].isoformat() if isinstance(image_data['created_at'], datetime) else image_data['created_at']
            }
            
            # æ·»åŠ mocké¢„æµ‹ä¿¡æ¯
            mock_prediction = {
                "location": image_data.get('location', 'London, UK'),
                "input_data": {
                    "location_name": image_data.get('location', 'London, UK'),
                    "temperature": 15.0,
                    "humidity": 60.0,
                    "pressure": 1013.0,
                    "wind_speed": 5.0
                },
                "result_data": {
                    "city": image_data.get('location', 'London, UK').split(',')[0],
                    "climate_score": 0.75,
                    "geographic_score": 0.80,
                    "economic_score": 0.70
                }
            }
            image_info["prediction"] = mock_prediction
            
            images.append(image_info)
        
        logger.info(f"Retrieved {len(images)} images from local storage with mock prediction data")
    
    # å¦‚æœä»ç„¶æ²¡æœ‰å›¾ç‰‡ï¼Œè¿”å›ç©ºåˆ—è¡¨
    if not images:
        logger.info("No images found in database or local storage")
    
    return jsonify({
        "success": True,
        "images": images,
        "count": len(images),
        "source": "database_with_predictions" if images and not LOCAL_IMAGES_STORE else "local_storage_with_mock" if LOCAL_IMAGES_STORE else "empty",
        "timestamp": datetime.now().isoformat()
    })

@images_bp.route('/<int:image_id>', methods=['GET'])
def get_image_detail(image_id):
    """
    è·å–å•å¼ å›¾ç‰‡è¯¦æƒ…APIç«¯ç‚¹
    
    è¿”å›: å›¾ç‰‡ä¿¡æ¯åŠå…³è”çš„é¢„æµ‹æ•°æ®
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # è”æŸ¥å›¾ç‰‡å’Œé¢„æµ‹æ•°æ®
        cur.execute("""
            SELECT 
                i.id, i.url, i.thumbnail_url, i.description, i.created_at,
                p.id as prediction_id, p.input_data, p.result_data, p.prompt, p.location
            FROM images i
            LEFT JOIN predictions p ON i.prediction_id = p.id
            WHERE i.id = %s
        """, (image_id,))
        
        row = cur.fetchone()
        if not row:
            return jsonify({
                "success": False,
                "error": "Image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        image_detail = {
            "id": row[0],
            "url": row[1],
            "thumbnail_url": row[2],
            "description": row[3],
            "created_at": row[4].isoformat(),
            "prediction": {
                "id": row[5],
                "input_data": row[6],
                "result_data": row[7],
                "prompt": row[8],
                "location": row[9]
            } if row[5] else None
        }
        
        cur.close()
        conn.close()
        
        return jsonify({
            "success": True,
            "image": image_detail,
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error fetching image detail: {e}")
        
        # æœ¬åœ°å¼€å‘æ¨¡å¼ï¼šå½“æ•°æ®åº“ä¸å¯ç”¨æ—¶ï¼Œæ£€æŸ¥æœ¬åœ°å­˜å‚¨æˆ–ç”ŸæˆåŠ¨æ€æ•°æ®
        if "nodename nor servname provided" in str(e) or "could not translate host name" in str(e):
            logger.info(f"Database unavailable - generating dynamic analysis for image {image_id}")
            
            # é¦–å…ˆæ£€æŸ¥æœ¬åœ°å­˜å‚¨
            if image_id in LOCAL_IMAGES_STORE:
                local_image = LOCAL_IMAGES_STORE[image_id]
                logger.info(f"Found image in local storage: {local_image['url']}")
                
                # ä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆå”¯ä¸€çš„åˆ†æç»“æœ
                dynamic_analysis = generate_dynamic_image_analysis(image_id, local_image)
                
                return jsonify({
                    "success": True,
                    "image": {
                        "id": local_image['id'],
                        "url": local_image['url'],
                        "thumbnail_url": local_image.get('thumbnail_url', local_image['url']),
                        "description": local_image['description'],
                        "created_at": local_image['created_at'].isoformat(),
                        "prediction": dynamic_analysis
                    },
                    "timestamp": datetime.now().isoformat(),
                    "mode": "local_storage_with_dynamic_analysis"
                }), 200
            
            # å¦‚æœæœ¬åœ°å­˜å‚¨ä¸­æ²¡æœ‰ï¼Œç”ŸæˆåŠ¨æ€æ¨¡æ‹Ÿæ•°æ®ï¼ˆæ¯å¼ å›¾ç‰‡ä¸åŒï¼‰
            logger.info(f"Image {image_id} not found in local storage - generating dynamic mock data")
            
            # ç”ŸæˆåŸºäºimage_idçš„åŠ¨æ€åˆ†ææ•°æ®
            dynamic_analysis = generate_dynamic_image_analysis(image_id)
            
            # åŠ¨æ€æ¨¡æ‹Ÿå›¾ç‰‡è¯¦æƒ…æ•°æ®ï¼ˆæ¯å¼ å›¾ç‰‡ä¸åŒï¼‰
            mock_image_detail = {
                "id": image_id,
                "url": f"https://res.cloudinary.com/dvbqtwgko/image/upload/v1752310322/obscura_images/mock_image_{image_id}.png",
                "thumbnail_url": f"https://res.cloudinary.com/dvbqtwgko/image/upload/v1752310322/obscura_images/mock_image_{image_id}.png",
                "description": f"Telescope generated artwork #{image_id}",
                "created_at": datetime.now().isoformat(),
                "prediction": dynamic_analysis
            }
            
            return jsonify({
                "success": True,
                "image": mock_image_detail,
                "timestamp": datetime.now().isoformat(),
                "mode": "dynamic_mock_data_for_local_development"
            }), 200
        
        # å…¶ä»–æ•°æ®åº“é”™è¯¯
        return jsonify({
            "success": False,
            "error": "Failed to fetch image detail",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/<int:image_id>/analysis', methods=['GET'])
def get_image_analysis(image_id):
    """
    è·å–å›¾ç‰‡è¯¦ç»†åˆ†ææ•°æ®APIç«¯ç‚¹
    
    è¿”å›: å›¾ç‰‡çš„æ·±åº¦åˆ†ææ•°æ®ï¼ŒåŒ…æ‹¬ç¯å¢ƒé¢„æµ‹ã€ç½®ä¿¡åº¦ç­‰
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # æŸ¥è¯¢å›¾ç‰‡å’Œå®Œæ•´é¢„æµ‹æ•°æ®
        cur.execute("""
            SELECT 
                i.id, i.url, i.thumbnail_url, i.description, i.created_at,
                p.id as prediction_id, p.input_data, p.result_data, p.prompt, p.location, p.created_at as prediction_created_at
            FROM images i
            LEFT JOIN predictions p ON i.prediction_id = p.id
            WHERE i.id = %s
        """, (image_id,))
        
        row = cur.fetchone()
        if not row:
            return jsonify({
                "success": False,
                "error": "Image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        # è§£æé¢„æµ‹ç»“æœæ•°æ®
        result_data = row[7] if row[7] else {}
        input_data = row[6] if row[6] else {}
        
        # æ„å»ºåˆ†æå“åº”æ•°æ®
        analysis_data = {
            "image": {
                "id": row[0],
                "url": row[1],
                "thumbnail_url": row[2],
                "description": row[3],
                "created_at": row[4].isoformat(),
                "file_size": _estimate_file_size(row[1]),
                "resolution": _estimate_resolution(row[1])
            },
            "prediction": {
                "id": row[5],
                "prompt": row[8],
                "location": row[9],
                "created_at": row[10].isoformat() if row[10] else None,
                "processing_time": _calculate_processing_time(row[4], row[10]) if row[10] else None
            } if row[5] else None,
            "analysis": {
                "environment_type": _extract_environment_type(result_data),
                "climate_prediction": _extract_climate_data(result_data),
                "vegetation_index": _extract_vegetation_data(result_data),
                "urban_development": _extract_urban_data(result_data),
                "confidence_scores": _extract_confidence_scores(result_data),
                "technical_details": _extract_technical_details(input_data, result_data)
            },
            "visualization_data": {
                "trend_data": _generate_trend_data(result_data),
                "confidence_chart": _generate_confidence_chart_data(result_data),
                "process_flow": _generate_process_flow_data()
            }
        }
        
        cur.close()
        conn.close()
        
        return jsonify({
            "success": True,
            "analysis": analysis_data,
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error fetching image analysis: {e}")
        return jsonify({
            "success": False,
            "error": "Failed to fetch image analysis",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/<int:image_id>/shap-test', methods=['GET'])
def test_shap_endpoint(image_id):
    """
    ç®€åŒ–çš„SHAPæµ‹è¯•ç«¯ç‚¹
    """
    try:
        return jsonify({
            "success": True,
            "message": "SHAP test endpoint working",
            "image_id": image_id,
            "timestamp": datetime.now().isoformat()
        }), 200
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/<int:image_id>/shap-analysis', methods=['GET'])
def get_image_shap_analysis(image_id):
    """
    è·å–å›¾ç‰‡çš„SHAPåˆ†ææ•°æ®APIç«¯ç‚¹
    
    ä¼˜å…ˆä»predictions.result_dataè¯»å–å·²å­˜å‚¨çš„åˆ†æç»“æœ
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # æŸ¥è¯¢å›¾ç‰‡å’Œå…³è”çš„é¢„æµ‹æ•°æ®
        cur.execute("""
            SELECT 
                i.id, i.url, i.description,
                p.input_data, p.result_data, p.location
            FROM images i
            LEFT JOIN predictions p ON i.prediction_id = p.id
            WHERE i.id = %s
        """, (image_id,))
        
        row = cur.fetchone()
        if not row:
            return jsonify({
                "success": False,
                "error": "Image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        # æå–æ•°æ®
        input_data = row[3] if row[3] else {}
        result_data = row[4] if row[4] else {}
        location = row[5] if row[5] else "Unknown Location"
        
        cur.close()
        conn.close()
        
        # æ£€æŸ¥result_dataä¸­æ˜¯å¦å·²æœ‰å®Œæ•´çš„SHAPåˆ†æç»“æœ
        if result_data and 'ai_story' in result_data and 'climate_score' in result_data:
            logger.info(f"âœ… Retrieved stored SHAP analysis for image {image_id}")
            
            # æ„å»ºSHAPåˆ†ææ•°æ®ç»“æ„
            shap_analysis_data = {
                "climate_score": result_data.get('climate_score', 0.5),
                "geographic_score": result_data.get('geographic_score', 0.5),
                "economic_score": result_data.get('economic_score', 0.5),
                "final_score": result_data.get('final_score', 0.5),
                "city": result_data.get('city', 'Unknown'),
                "shap_analysis": result_data.get('shap_analysis', {}),
                "confidence": result_data.get('confidence', 1.0)
            }
            
            # è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„
            enhanced_shap_analysis = transform_to_hierarchical_shap_data(shap_analysis_data)
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            validation_result = validate_hierarchical_shap_data(enhanced_shap_analysis)
            
            return jsonify({
                "success": True,
                "data": {
                    **enhanced_shap_analysis,
                    'ai_story': result_data.get('ai_story'),
                    "integration_metadata": {
                        "analysis_timestamp": result_data.get('analysis_metadata', {}).get('generated_at', datetime.now().isoformat()),
                        "model_version": result_data.get('analysis_metadata', {}).get('model_version', 'shap_v1.0.0'),
                        "analysis_source": "stored_in_predictions",
                        "note": "Pre-generated SHAP analysis from image upload",
                        "data_format_version": "1.2.0",
                        "fallback_used": result_data.get('analysis_metadata', {}).get('fallback_used', False)
                    },
                    "data_validation": validation_result
                },
                "timestamp": datetime.now().isoformat(),
                "mode": "stored_analysis"
            }), 200
        
        # å¦‚æœæ²¡æœ‰å­˜å‚¨çš„å®Œæ•´åˆ†æç»“æœï¼Œå›é€€åˆ°å®æ—¶ç”Ÿæˆ
        logger.info(f"ğŸ”„ No complete SHAP analysis found, generating fallback for image {image_id}")
        
        # å°è¯•ä»æ—§çš„image_analysisè¡¨è·å–æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰
        try:
            conn = psycopg2.connect(os.environ['DATABASE_URL'])
            cur = conn.cursor()
            
            cur.execute("""
                SELECT shap_data, ai_story, generated_at 
                FROM image_analysis 
                WHERE image_id = %s
            """, (image_id,))
            
            analysis_row = cur.fetchone()
            
            if analysis_row:
                # è¿”å›å­˜å‚¨çš„åˆ†æç»“æœ
                shap_data, ai_story, generated_at = analysis_row
                
                logger.info(f"âœ… Retrieved legacy analysis for image {image_id}")
                
                cur.close()
                conn.close()
                
                # è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„
                enhanced_shap_analysis = transform_to_hierarchical_shap_data(json.loads(shap_data))
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                validation_result = validate_hierarchical_shap_data(enhanced_shap_analysis)
                
                return jsonify({
                    "success": True,
                    "data": {
                        **enhanced_shap_analysis,
                        'ai_story': json.loads(ai_story),
                        "integration_metadata": {
                            "analysis_timestamp": generated_at.isoformat(),
                            "model_version": "legacy_v1.1",
                            "analysis_source": "legacy_image_analysis_table",
                            "note": "Retrieved from legacy analysis table",
                            "data_format_version": "1.1.0"
                        },
                        "data_validation": validation_result
                    },
                    "timestamp": datetime.now().isoformat(),
                    "mode": "legacy_analysis"
                }), 200
            
            cur.close()
            conn.close()
            
        except Exception as legacy_error:
            logger.warning(f"âš ï¸ Legacy analysis table unavailable: {legacy_error}")
        
        # æœ€ç»ˆå›é€€ï¼šç”Ÿæˆå®æ—¶åˆ†æ
        logger.info(f"ğŸ”„ Generating real-time analysis for image {image_id}")
        
        # ç”Ÿæˆæ–°çš„SHAPåˆ†ææ•°æ®
        shap_data = generate_shap_analysis_data(image_id, row[2])
        
        # ç”ŸæˆAIæ•…äº‹
        story_data = generate_ai_environmental_story(shap_data)
        
        # è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„
        enhanced_shap_analysis = transform_to_hierarchical_shap_data(shap_data)
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        validation_result = validate_hierarchical_shap_data(enhanced_shap_analysis)
        
        # æ„å»ºå®Œæ•´çš„å“åº”æ•°æ®
        real_time_analysis = {
            **enhanced_shap_analysis,
            'ai_story': story_data,
            "integration_metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "model_version": "real_time_v1.2",
                "analysis_source": "generated_on_demand",
                "note": "Generated real-time SHAP analysis",
                "data_format_version": "1.2.0",
                "warning": "This analysis is generated each time and may vary"
            },
            "data_validation": validation_result
        }
        
        return jsonify({
            "success": True,
            "data": real_time_analysis,
            "timestamp": datetime.now().isoformat(),
            "mode": "real_time_generation"
        }), 200
        
    except Exception as e:
        logger.error(f"Error in SHAP analysis endpoint: {e}")
        
        # æ•°æ®åº“ä¸å¯ç”¨æ—¶çš„æœ€ç»ˆå›é€€
        database_issues = [
            "nodename nor servname provided",
            "could not translate host name",
            "relation \"image_analysis\" does not exist",
            "relation \"predictions\" does not exist",
            "does not exist"
        ]
        
        is_database_issue = any(issue in str(e) for issue in database_issues)
        
        if is_database_issue:
            logger.info(f"Database issue detected - using local fallback for image {image_id}")
            
            # æ£€æŸ¥æœ¬åœ°åˆ†æå­˜å‚¨
            if image_id in LOCAL_ANALYSIS_STORE:
                stored_analysis = LOCAL_ANALYSIS_STORE[image_id]
                logger.info(f"âœ… Retrieved local analysis for image {image_id}")
                
                # è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„
                enhanced_shap_analysis = transform_to_hierarchical_shap_data(stored_analysis['shap_analysis'])
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                validation_result = validate_hierarchical_shap_data(enhanced_shap_analysis)
                
                return jsonify({
                    "success": True,
                    "data": {
                        **enhanced_shap_analysis,
                        'ai_story': stored_analysis['ai_story'],
                        "integration_metadata": {
                            "analysis_timestamp": stored_analysis.get('generated_at', datetime.now().isoformat()),
                            "model_version": "local_storage_v1.0",
                            "analysis_source": "local_fallback",
                            "note": "Retrieved from local storage due to database unavailability",
                            "data_format_version": "1.0.0"
                        },
                        "data_validation": validation_result
                    },
                    "timestamp": datetime.now().isoformat(),
                    "mode": "local_fallback"
                }), 200
            
            # æœ€ç»ˆçš„é”™è¯¯å“åº”
            return jsonify({
                "success": False,
                "error": "Analysis data temporarily unavailable",
                "details": "Database and local storage unavailable",
                "timestamp": datetime.now().isoformat()
            }), 503
        
        # å…¶ä»–é”™è¯¯
        return jsonify({
            "success": False,
            "error": "Failed to retrieve SHAP analysis",
            "details": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/<int:image_id>/download', methods=['GET'])
def download_image(image_id):
    """
    å›¾ç‰‡ä¸‹è½½APIç«¯ç‚¹
    
    è¿”å›: å›¾ç‰‡æ–‡ä»¶æµä¾›ç›´æ¥ä¸‹è½½
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # æŸ¥è¯¢å›¾ç‰‡URL
        cur.execute("SELECT url, description FROM images WHERE id = %s", (image_id,))
        row = cur.fetchone()
        
        if not row:
            return jsonify({
                "success": False,
                "error": "Image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        image_url = row[0]
        description = row[1] or f"obscura_image_{image_id}"
        
        cur.close()
        conn.close()
        
        # ä»Cloudinaryè·å–å›¾ç‰‡æ–‡ä»¶æµ
        import requests
        response = requests.get(image_url, stream=True, timeout=30)
        
        if response.status_code != 200:
            logger.error(f"Failed to fetch image from Cloudinary: {response.status_code}")
            return jsonify({
                "success": False,
                "error": "Failed to fetch image from storage",
                "timestamp": datetime.now().isoformat()
            }), 500
        
        # è®¾ç½®ä¸‹è½½å“åº”å¤´
        from flask import Response
        import re
        
        # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤æ‰€æœ‰æ— æ•ˆå­—ç¬¦
        safe_description = re.sub(r'[^\w\s-]', '', description or f"obscura_image_{image_id}")
        safe_description = re.sub(r'\s+', '_', safe_description.strip())
        filename = f"{safe_description[:50]}.jpg"  # é™åˆ¶é•¿åº¦
        
        # åˆ›å»ºæµå¼å“åº”
        def generate():
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    yield chunk
        
        logger.info(f"Image download streaming for ID: {image_id}")
        
        return Response(
            generate(),
            content_type='image/jpeg',
            headers={
                'Content-Disposition': f'attachment; filename="{filename}"',
                'Content-Type': 'image/jpeg',
                'Cache-Control': 'no-cache'
            }
        )
        
    except Exception as e:
        logger.error(f"Error downloading image: {e}")
        
        # æ•°æ®åº“ä¸å¯ç”¨æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
        database_issues = [
            "nodename nor servname provided",
            "could not translate host name",
            "relation \"images\" does not exist",
            "does not exist"
        ]
        
        is_database_issue = any(issue in str(e) for issue in database_issues)
        
        if is_database_issue:
            # æ£€æŸ¥æœ¬åœ°å­˜å‚¨
            if image_id in LOCAL_IMAGES_STORE:
                local_image = LOCAL_IMAGES_STORE[image_id]
                image_url = local_image['url']
                description = local_image['description'] or f"obscura_image_{image_id}"
                
                try:
                    import requests
                    response = requests.get(image_url, stream=True, timeout=30)
                    
                    if response.status_code == 200:
                        # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤æ‰€æœ‰æ— æ•ˆå­—ç¬¦
                        safe_description = re.sub(r'[^\w\s-]', '', description or f"obscura_image_{image_id}")
                        safe_description = re.sub(r'\s+', '_', safe_description.strip())
                        filename = f"{safe_description[:50]}.jpg"  # é™åˆ¶é•¿åº¦
                        
                        def generate():
                            for chunk in response.iter_content(chunk_size=8192):
                                if chunk:
                                    yield chunk
                        
                        logger.info(f"Image download streaming from local storage for ID: {image_id}")
                        
                        from flask import Response
                        return Response(
                            generate(),
                            content_type='image/jpeg',
                            headers={
                                'Content-Disposition': f'attachment; filename="{filename}"',
                                'Content-Type': 'image/jpeg',
                                'Cache-Control': 'no-cache'
                            }
                        )
                except Exception as fallback_error:
                    logger.error(f"Fallback download failed: {fallback_error}")
        
        return jsonify({
            "success": False,
            "error": "Failed to download image",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/navigation/<int:image_id>', methods=['GET'])
def get_image_navigation(image_id):
    """
    è·å–å›¾ç‰‡å¯¼èˆªä¿¡æ¯APIç«¯ç‚¹
    
    è¿”å›: ä¸Šä¸€å¼ å’Œä¸‹ä¸€å¼ å›¾ç‰‡çš„ä¿¡æ¯
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # è·å–å½“å‰å›¾ç‰‡çš„åˆ›å»ºæ—¶é—´
        cur.execute("SELECT created_at FROM images WHERE id = %s", (image_id,))
        current_row = cur.fetchone()
        
        if not current_row:
            return jsonify({
                "success": False,
                "error": "Current image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        current_created_at = current_row[0]
        
        # è·å–ä¸Šä¸€å¼ å›¾ç‰‡ï¼ˆæ›´æ—©çš„ï¼‰
        cur.execute("""
            SELECT id, url, thumbnail_url, description 
            FROM images 
            WHERE created_at < %s 
            ORDER BY created_at DESC 
            LIMIT 1
        """, (current_created_at,))
        prev_row = cur.fetchone()
        
        # è·å–ä¸‹ä¸€å¼ å›¾ç‰‡ï¼ˆæ›´æ™šçš„ï¼‰
        cur.execute("""
            SELECT id, url, thumbnail_url, description 
            FROM images 
            WHERE created_at > %s 
            ORDER BY created_at ASC 
            LIMIT 1
        """, (current_created_at,))
        next_row = cur.fetchone()
        
        cur.close()
        conn.close()
        
        navigation_data = {
            "current_id": image_id,
            "previous": {
                "id": prev_row[0],
                "url": prev_row[1],
                "thumbnail_url": prev_row[2],
                "description": prev_row[3]
            } if prev_row else None,
            "next": {
                "id": next_row[0],
                "url": next_row[1],
                "thumbnail_url": next_row[2],
                "description": next_row[3]
            } if next_row else None
        }
        
        return jsonify({
            "success": True,
            "navigation": navigation_data,
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error fetching navigation data: {e}")
        return jsonify({
            "success": False,
            "error": "Failed to fetch navigation data",
            "timestamp": datetime.now().isoformat()
        }), 500

# Helper functions for data processing
def _estimate_file_size(image_url):
    """ä¼°ç®—å›¾ç‰‡æ–‡ä»¶å¤§å°"""
    try:
        import requests
        response = requests.head(image_url, timeout=5)
        content_length = response.headers.get('Content-Length')
        if content_length:
            size_bytes = int(content_length)
            if size_bytes < 1024:
                return f"{size_bytes} B"
            elif size_bytes < 1024 * 1024:
                return f"{size_bytes / 1024:.1f} KB"
            else:
                return f"{size_bytes / (1024 * 1024):.1f} MB"
    except:
        pass
    return "Unknown"

def _estimate_resolution(image_url):
    """ä¼°ç®—å›¾ç‰‡åˆ†è¾¨ç‡"""
    try:
        # ä»Cloudinary URLä¸­æå–å˜æ¢ä¿¡æ¯ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
        return "1024x1024"  # é»˜è®¤åˆ†è¾¨ç‡
    except:
        return "Unknown"

def _calculate_processing_time(image_created, prediction_created):
    """è®¡ç®—å¤„ç†æ—¶é—´"""
    try:
        if image_created and prediction_created:
            delta = abs((image_created - prediction_created).total_seconds())
            if delta < 60:
                return f"{delta:.1f} seconds"
            else:
                return f"{delta / 60:.1f} minutes"
    except:
        pass
    return "Unknown"

def _extract_environment_type(result_data):
    """ä»ç»“æœæ•°æ®ä¸­æå–ç¯å¢ƒç±»å‹"""
    if not result_data:
        return {"type": "Unknown", "confidence": 0}
    
    # æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´
    return {
        "type": result_data.get("environment", "Mixed Environment"),
        "confidence": result_data.get("environment_confidence", 85.5)
    }

def _extract_climate_data(result_data):
    """æå–æ°”å€™é¢„æµ‹æ•°æ®"""
    if not result_data:
        return {"prediction": "Unknown", "confidence": 0}
    
    return {
        "prediction": result_data.get("climate", "Temperate"),
        "confidence": result_data.get("climate_confidence", 78.3)
    }

def _extract_vegetation_data(result_data):
    """æå–æ¤è¢«æŒ‡æ•°æ•°æ®"""
    if not result_data:
        return {"index": "Unknown", "confidence": 0}
    
    return {
        "index": result_data.get("vegetation", "Moderate"),
        "confidence": result_data.get("vegetation_confidence", 92.1)
    }

def _extract_urban_data(result_data):
    """æå–åŸå¸‚å‘å±•æ•°æ®"""
    if not result_data:
        return {"level": "Unknown", "confidence": 0}
    
    return {
        "level": result_data.get("urban_development", "Low"),
        "confidence": result_data.get("urban_confidence", 67.8)
    }

def _extract_confidence_scores(result_data):
    """æå–æ‰€æœ‰ç½®ä¿¡åº¦åˆ†æ•°"""
    if not result_data:
        return {}
    
    return {
        "overall": result_data.get("overall_confidence", 80.0),
        "environment": result_data.get("environment_confidence", 85.5),
        "climate": result_data.get("climate_confidence", 78.3),
        "vegetation": result_data.get("vegetation_confidence", 92.1),
        "urban": result_data.get("urban_confidence", 67.8)
    }

def _extract_technical_details(input_data, result_data):
    """æå–æŠ€æœ¯ç»†èŠ‚"""
    return {
        "model": {
            "architecture": "DALL-E 3",
            "version": "v3.0",
            "training_data": "GPT-4 Vision",
            "accuracy": "92.3%"
        },
        "processing": {
            "processing_time": "2.3 seconds",
            "memory_usage": "1.2 GB",
            "gpu_utilization": "78%",
            "batch_size": "1"
        },
        "environment": {
            "location": input_data.get("location", "Global"),
            "time_period": "2024",
            "climate_zone": "Temperate",
            "data_sources": "OpenWeather, Satellite Data"
        }
    }

def _generate_trend_data(result_data):
    """ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨æ•°æ®"""
    return {
        "labels": ["Jan", "Feb", "Mar", "Apr", "May", "Jun"],
        "datasets": [{
            "label": "Environmental Score",
            "data": [65, 69, 72, 78, 82, 85],
            "borderColor": "#CD853F",
            "backgroundColor": "rgba(205, 133, 63, 0.1)"
        }]
    }

def _generate_confidence_chart_data(result_data):
    """ç”Ÿæˆç½®ä¿¡åº¦å›¾è¡¨æ•°æ®"""
    confidence_scores = _extract_confidence_scores(result_data)
    return {
        "labels": ["Environment", "Climate", "Vegetation", "Urban"],
        "datasets": [{
            "label": "Confidence %",
            "data": [
                confidence_scores.get("environment", 85.5),
                confidence_scores.get("climate", 78.3),
                confidence_scores.get("vegetation", 92.1),
                confidence_scores.get("urban", 67.8)
            ],
            "backgroundColor": [
                "rgba(255, 191, 0, 0.8)",
                "rgba(205, 133, 63, 0.8)",
                "rgba(160, 82, 45, 0.8)",
                "rgba(139, 69, 19, 0.8)"
            ]
        }]
    }

def _generate_process_flow_data():
    """ç”Ÿæˆå¤„ç†æµç¨‹å›¾æ•°æ®"""
    return {
        "nodes": [
            {"id": "input", "label": "User Input", "x": 50, "y": 100},
            {"id": "weather", "label": "Weather API", "x": 200, "y": 50},
            {"id": "ai", "label": "AI Processing", "x": 350, "y": 100},
            {"id": "output", "label": "Image Generation", "x": 500, "y": 100}
        ],
        "edges": [
            {"from": "input", "to": "weather"},
            {"from": "input", "to": "ai"},
            {"from": "weather", "to": "ai"},
            {"from": "ai", "to": "output"}
        ]
    }

def _determine_climate_type(shap_data):
    """æ ¹æ®SHAPåˆ†æç»“æœç¡®å®šæ°”å€™ç±»å‹"""
    climate_score = shap_data.get('climate_score', 0.5)
    city = shap_data.get('city', 'Unknown')
    
    if climate_score > 0.8:
        return "optimal"
    elif climate_score > 0.6:
        return "temperate"
    elif climate_score > 0.4:
        return "moderate"
    else:
        return "challenging"

def _calculate_vegetation_index(shap_data):
    """æ ¹æ®SHAPåˆ†æè®¡ç®—æ¤è¢«æŒ‡æ•°"""
    geographic_score = shap_data.get('geographic_score', 0.5)
    climate_score = shap_data.get('climate_score', 0.5)
    
    # ç®€åŒ–çš„æ¤è¢«æŒ‡æ•°è®¡ç®—ï¼ŒåŸºäºåœ°ç†å’Œæ°”å€™è¯„åˆ†
    vegetation_index = (geographic_score * 0.6 + climate_score * 0.4)
    return max(0.0, min(1.0, vegetation_index))

def _generate_short_term_prediction(shap_data):
    """ç”ŸæˆçŸ­æœŸé¢„æµ‹æ–‡æœ¬"""
    final_score = shap_data.get('final_score', 0.5)
    city = shap_data.get('city', 'Unknown')
    
    if final_score > 0.7:
        return f"Favorable environmental conditions expected for {city} region"
    elif final_score > 0.5:
        return f"Moderate environmental stability predicted for {city}"
    else:
        return f"Variable environmental conditions forecasted for {city}"

def _generate_long_term_prediction(shap_data):
    """ç”Ÿæˆé•¿æœŸé¢„æµ‹æ–‡æœ¬"""
    climate_score = shap_data.get('climate_score', 0.5)
    geographic_score = shap_data.get('geographic_score', 0.5)
    
    if climate_score > 0.6 and geographic_score > 0.6:
        return "Long-term environmental resilience indicated"
    elif climate_score > 0.4 or geographic_score > 0.4:
        return "Moderate long-term environmental stability expected"
    else:
        return "Environmental monitoring recommended for long-term planning"

def _create_fallback_result_data(environmental_data):
    """åˆ›å»ºå›é€€çš„result_dataç»“æ„ï¼ˆå½“SHAP APIä¸å¯ç”¨æ—¶ï¼‰"""
    
    # åŸºäºä½ç½®çš„ç®€åŒ–è¯„åˆ†
    latitude = environmental_data.get('latitude', 51.5074)
    longitude = environmental_data.get('longitude', -0.1278) 
    temperature = environmental_data.get('temperature', 15.0)
    humidity = environmental_data.get('humidity', 60.0)
    
    # ç®€åŒ–çš„è¯„åˆ†ç®—æ³•
    climate_score = max(0.2, min(1.0, 0.5 + (temperature - 10) / 40))  # åŸºäºæ¸©åº¦
    geographic_score = max(0.3, min(1.0, 0.6 + (50 - latitude) / 100))  # åŸºäºçº¬åº¦
    economic_score = 0.5  # é»˜è®¤å€¼
    final_score = climate_score * 0.4 + geographic_score * 0.35 + economic_score * 0.25
    
    # ç¡®å®šæœ€è¿‘åŸå¸‚
    city_centers = {
        'London': {'lat': 51.5074, 'lon': -0.1278},
        'Manchester': {'lat': 53.4808, 'lon': -2.2426},
        'Edinburgh': {'lat': 55.9533, 'lon': -3.1883}
    }
    
    closest_city = "Unknown"
    min_distance = float('inf')
    for city, coords in city_centers.items():
        distance = ((latitude - coords['lat'])**2 + (longitude - coords['lon'])**2)**0.5
        if distance < min_distance:
            min_distance = distance
            closest_city = city
    
    # ğŸ”§ ä¿®å¤ï¼šå³ä½¿åœ¨fallbackæ¨¡å¼ä¸‹ä¹Ÿç”ŸæˆåŠ¨æ€AIæ•…äº‹
    try:
        # æ„å»ºSHAPæ•°æ®ç”¨äºæ•…äº‹ç”Ÿæˆ
        fallback_shap_data = {
            'climate_score': climate_score,
            'geographic_score': geographic_score,
            'economic_score': economic_score,
            'final_score': final_score,
            'city': closest_city,
            'shap_analysis': {
                'feature_importance': {
                    'temperature': 0.25,
                    'humidity': 0.20,
                    'location': 0.30,
                    'seasonal': 0.25
                }
            },
            'overall_confidence': 0.7,
            'temperature': temperature,
            'humidity': humidity
        }
        
        # è°ƒç”¨DeepSeekç”ŸæˆåŠ¨æ€æ•…äº‹
        logger.info(f"ğŸ”„ Generating AI story in fallback mode for {closest_city}")
        generated_story = generate_ai_environmental_story(fallback_shap_data, force_unique=True)
        
        # å¦‚æœæ•…äº‹ç”ŸæˆæˆåŠŸï¼Œæ·»åŠ fallbackæ ‡è¯†
        if generated_story and not generated_story.startswith("Environmental analysis temporarily"):
            ai_story = f"[Simplified Analysis] {generated_story}"
            logger.info(f"âœ… AI story generated successfully in fallback mode for {closest_city}")
        else:
            # å¦‚æœDeepSeekä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨åŠ¨æ€fallbackæ•…äº‹
            ai_story = _generate_dynamic_fallback_story(fallback_shap_data)
            logger.warning(f"âš ï¸ Using dynamic fallback story for {closest_city}")
            
    except Exception as e:
        logger.error(f"âŒ Failed to generate AI story in fallback mode: {e}")
        # æœ€åçš„å…œåº•ï¼šåŠ¨æ€fallbackæ•…äº‹
        ai_story = _generate_dynamic_fallback_story({
            'climate_score': climate_score,
            'geographic_score': geographic_score,
            'economic_score': economic_score,
            'city': closest_city,
            'temperature': temperature,
            'humidity': humidity
        })
    
    return {
        # å…¼å®¹å­—æ®µ
        "temperature": temperature,
        "humidity": humidity,
        "confidence": 0.7,  # é™ä½ç½®ä¿¡åº¦ä»¥è¡¨æ˜è¿™æ˜¯å›é€€æ•°æ®
        "climate_type": "temperate",
        "vegetation_index": 0.6,
        "predictions": {
            "short_term": "Fallback prediction - moderate conditions",
            "long_term": "Long-term analysis requires SHAP model"
        },
        
        # SHAPå­—æ®µï¼ˆå›é€€å€¼ï¼‰
        "climate_score": climate_score,
        "geographic_score": geographic_score,
        "economic_score": economic_score,
        "final_score": final_score,
        "city": closest_city,
        "shap_analysis": {
            "feature_importance": {
                "temperature": 0.25,
                "humidity": 0.20,
                "location": 0.30,
                "seasonal": 0.25
            },
            "note": "Simplified analysis - SHAP model unavailable"
        },
        "ai_story": ai_story,  # ğŸ”§ ç°åœ¨ä½¿ç”¨åŠ¨æ€ç”Ÿæˆçš„æ•…äº‹
        
        # å…ƒæ•°æ®
        "analysis_metadata": {
            "generated_at": datetime.now().isoformat(),
            "model_version": "fallback_v1.0.0",
            "api_source": "fallback_algorithm",
            "fallback_used": True,
            "fallback_reason": "SHAP API unavailable",
            "story_generation": "dynamic_ai_story" if ai_story and not ai_story.startswith("In a world") else "static_fallback"
        }
    }

def _generate_dynamic_fallback_story(shap_data):
    """
    ç”ŸæˆåŠ¨æ€çš„fallbackæ•…äº‹ï¼ˆå½“DeepSeek APIä¹Ÿä¸å¯ç”¨æ—¶ï¼‰
    ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½äº§ç”Ÿä¸åŒçš„æ•…äº‹
    """
    import random
    import time
    
    # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡éƒ½ä¸åŒ
    random.seed(int(time.time() * 1000000) % 2**32)
    
    climate_score = shap_data.get('climate_score', 0.5) * 100
    geographic_score = shap_data.get('geographic_score', 0.5) * 100
    economic_score = shap_data.get('economic_score', 0.5) * 100
    city = shap_data.get('city', 'Unknown Location')
    temperature = shap_data.get('temperature', 20)
    humidity = shap_data.get('humidity', 60)
    
    # åŠ¨æ€æ•…äº‹å¼€å¤´
    story_openings = [
        f"In a world where {city} stands at the crossroads of environmental change",
        f"Beneath the shifting skies of {city}, nature tells its ancient story",
        f"The winds of change sweep through {city}, carrying whispers of transformation",
        f"In the heart of {city}, where urban landscapes meet natural forces",
        f"Time flows differently in {city}, where each season brings new revelations",
        f"Hidden within {city}'s environmental tapestry lies a complex narrative",
        f"The atmospheric symphony of {city} plays a unique composition",
        f"Between the earth and sky, {city} experiences its own environmental dance"
    ]
    
    # åŠ¨æ€æè¿°ç‰‡æ®µ
    climate_descriptions = [
        f"climate patterns pulse with {climate_score:.1f}% intensity",
        f"atmospheric conditions weave stories of {climate_score:.1f}% complexity",
        f"weather systems demonstrate {climate_score:.1f}% environmental vigor",
        f"climatic forces exhibit {climate_score:.1f}% natural resilience"
    ]
    
    geographic_descriptions = [
        f"while geographic influences shape {geographic_score:.1f}% of the landscape",
        f"as topographical elements contribute {geographic_score:.1f}% to the regional character",
        f"where geological foundations provide {geographic_score:.1f}% environmental stability",
        f"through terrain features that deliver {geographic_score:.1f}% spatial dynamics"
    ]
    
    economic_descriptions = [
        f"Economic currents flow at {economic_score:.1f}% capacity",
        f"Socioeconomic patterns influence {economic_score:.1f}% of development",
        f"Human activities contribute {economic_score:.1f}% to environmental pressure",
        f"Development forces maintain {economic_score:.1f}% regional momentum"
    ]
    
    # åŠ¨æ€ç»“å°¾
    story_endings = [
        "creating a unique environmental signature that defines this moment in time.",
        "weaving together past, present, and future in an intricate ecological ballet.",
        "establishing patterns that will echo through generations of environmental change.",
        "forming bonds between human activity and natural systems that transcend simple analysis.",
        "generating ripples of environmental influence that extend far beyond visible boundaries.",
        "crafting a narrative where science meets poetry in nature's grand design.",
        "building bridges between measurable data and the immeasurable beauty of our world."
    ]
    
    # éšæœºç»„åˆç”Ÿæˆæ•…äº‹
    opening = random.choice(story_openings)
    climate_desc = random.choice(climate_descriptions)
    geo_desc = random.choice(geographic_descriptions)
    econ_desc = random.choice(economic_descriptions)
    ending = random.choice(story_endings)
    
    # æ·»åŠ æ¸©åº¦å’Œæ¹¿åº¦çš„åŠ¨æ€æè¿°
    temp_desc = "crisp" if temperature < 15 else "warm" if temperature < 25 else "heated"
    humidity_desc = "dry" if humidity < 40 else "balanced" if humidity < 70 else "moist"
    
    story = f"{opening}, {climate_desc}, {geo_desc}. {econ_desc}, while {temp_desc} {temperature}Â°C air carries {humidity_desc} {humidity}% humidity through the region, {ending}"
    
    # ç¡®ä¿æ•…äº‹é•¿åº¦åˆé€‚ï¼ˆå¤§çº¦100è¯ï¼‰
    words = story.split()
    if len(words) > 100:
        story = ' '.join(words[:100]) + "..."
    elif len(words) < 80:
        story += f" This environmental snapshot captures {city}'s unique character at this precise moment."
    
    return story

def generate_dynamic_image_analysis(image_id, local_image_data=None):
    """
    ä¸ºæ¯å¼ å›¾ç‰‡ç”ŸæˆåŠ¨æ€ã€ç‹¬ç‰¹çš„åˆ†æç»“æœ
    
    Args:
        image_id: å›¾ç‰‡ID
        local_image_data: æœ¬åœ°å›¾ç‰‡æ•°æ®ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        dict: åŒ…å«å®Œæ•´SHAPåˆ†æå’ŒAIæ•…äº‹çš„é¢„æµ‹æ•°æ®
    """
    import random
    import hashlib
    
    # ä½¿ç”¨image_idä½œä¸ºç§å­ï¼Œç¡®ä¿æ¯å¼ å›¾ç‰‡çš„ç»“æœä¸€è‡´ä½†ä¸åŒ
    random.seed(image_id)
    
    # ğŸ”§ ä¿®å¤ï¼šä»æ•°æ®åº“è·å–çœŸå®çš„ç”¨æˆ·è¾“å…¥åæ ‡
    # è€Œä¸æ˜¯åŸºäºimage_idéšæœºé€‰æ‹©åŸå¸‚
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # æŸ¥è¯¢å›¾ç‰‡å…³è”çš„é¢„æµ‹è®°å½•ï¼Œè·å–çœŸå®çš„ç”¨æˆ·è¾“å…¥åæ ‡
        cur.execute("""
            SELECT p.input_data, p.location
            FROM images i
            LEFT JOIN predictions p ON i.prediction_id = p.id
            WHERE i.id = %s
        """, (image_id,))
        
        row = cur.fetchone()
        cur.close()
        conn.close()
        
        if row and row[0]:
            # ä»é¢„æµ‹è®°å½•ä¸­è·å–çœŸå®çš„ç”¨æˆ·è¾“å…¥åæ ‡
            input_data = row[0]
            if isinstance(input_data, str):
                import json
                input_data = json.loads(input_data)
            
            # è·å–çœŸå®çš„ç»çº¬åº¦åæ ‡
            latitude = input_data.get('latitude')
            longitude = input_data.get('longitude')
            location_name = row[1] or "Unknown Location"
            
            if latitude is not None and longitude is not None:
                logger.info(f"âœ… ä½¿ç”¨çœŸå®ç”¨æˆ·è¾“å…¥åæ ‡: ({latitude}, {longitude}) - {location_name}")
            else:
                # å¦‚æœæ²¡æœ‰çœŸå®åæ ‡ï¼Œä½¿ç”¨é»˜è®¤çš„ä¼¦æ•¦åæ ‡
                latitude, longitude = 51.5074, -0.1278
                location_name = "London, UK"
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°çœŸå®åæ ‡ï¼Œä½¿ç”¨é»˜è®¤åæ ‡: ({latitude}, {longitude})")
        else:
            # å¦‚æœæ²¡æœ‰é¢„æµ‹è®°å½•ï¼Œä½¿ç”¨é»˜è®¤åæ ‡
            latitude, longitude = 51.5074, -0.1278
            location_name = "London, UK"
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°é¢„æµ‹è®°å½•ï¼Œä½¿ç”¨é»˜è®¤åæ ‡: ({latitude}, {longitude})")
            
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        # æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åæ ‡
        latitude, longitude = 51.5074, -0.1278
        location_name = "London, UK"
    
    # è·å–å½“å‰æœˆä»½
    current_month = datetime.now().month
    
    # æ„å»ºç¯å¢ƒæ•°æ®ï¼ˆç”¨äºè®°å½•ï¼‰
    environmental_data = {
        'latitude': latitude,
        'longitude': longitude,
        'month': current_month,
        'location_name': location_name,
        'timestamp': datetime.now().isoformat()
    }
    
    # ğŸ”§ ä¿®å¤ï¼šç›´æ¥è°ƒç”¨MLæ¨¡å‹è€Œä¸æ˜¯æ¨¡æ‹Ÿæ•°æ®
    try:
        logger.info(f"ğŸ”® Generating real ML prediction for image {image_id} at {location_name}")
        
        # å¯¼å…¥SHAPæ¨¡å‹
        from api.routes.shap_predict import get_shap_model
        model = get_shap_model()
        
        # ç›´æ¥è°ƒç”¨æ¨¡å‹é¢„æµ‹
        shap_result = model.predict_environmental_scores(
            latitude=latitude,
            longitude=longitude,
            month=current_month
        )
        
        if shap_result.get('success'):
            shap_data = shap_result
            logger.info(f"âœ… ML prediction successful for {location_name}")
            
            # åº”ç”¨åˆ†æ•°å½’ä¸€åŒ–
            from utils.score_normalizer import get_score_normalizer
            normalizer = get_score_normalizer()
            normalized_result = normalizer.normalize_shap_result(shap_data)
                
            # ç”ŸæˆAIæ•…äº‹
            ai_story = generate_ai_environmental_story(normalized_result)
            
            # æ„å»ºå®Œæ•´é¢„æµ‹æ•°æ®
            return {
                "id": image_id,
                "input_data": environmental_data,
                "result_data": {
                    # åŸºç¡€ç¯å¢ƒæ•°æ®
                    "temperature": normalized_result.get('climate_score', 0.5) * 40 + 10,  # è½¬æ¢ä¸ºæ¸©åº¦
                    "humidity": normalized_result.get('geographic_score', 0.5) * 60 + 30,  # è½¬æ¢ä¸ºæ¹¿åº¦
                    "confidence": normalized_result.get('overall_confidence', 0.85),
                    "climate_type": _determine_climate_type(normalized_result),
                    "vegetation_index": _calculate_vegetation_index(normalized_result),
                    "predictions": {
                        "short_term": _generate_short_term_prediction(normalized_result),
                        "long_term": _generate_long_term_prediction(normalized_result)
                    },
                    
                    # å®Œæ•´SHAPåˆ†æ
                    "climate_score": normalized_result.get('climate_score', 0.5),
                    "geographic_score": normalized_result.get('geographic_score', 0.5),
                    "economic_score": normalized_result.get('economic_score', 0.5),
                    "final_score": normalized_result.get('final_score', 0.5),
                    "city": normalized_result.get('city', location_name),
                    "shap_analysis": normalized_result.get('shap_analysis', {}),
                    "ai_story": ai_story,
                    
                    # åˆ†æå…ƒæ•°æ®
                    "analysis_metadata": {
                        "generated_at": datetime.now().isoformat(),
                        "model_version": "hybrid_ml_v1.0.0",
                        "api_source": "real_ml_prediction", 
                        "location": location_name,
                        "image_id": image_id,
                        "ml_models_used": ["RandomForest_climate", "LSTM_geographic"],
                        "coordinates_source": "user_input" if latitude != 51.5074 or longitude != -0.1278 else "default"
                    }
                },
                "prompt": f"AI environmental analysis for {location_name} based on telescope observation",
                "location": location_name
            }
        
    except Exception as e:
        logger.warning(f"âš ï¸ ML prediction failed for image {image_id}: {e}")
    
    # Fallback: ç”ŸæˆåŸºäºç¯å¢ƒæ•°æ®çš„æ¨¡æ‹ŸSHAPåˆ†æ
    logger.info(f"ğŸ”„ Generating fallback SHAP analysis for image {image_id}")
    
    # åŸºäºçœŸå®åæ ‡è®¡ç®—åˆ†æ•°
    climate_score = min(0.9, max(0.1, (latitude - 30) / 60 + random.uniform(-0.1, 0.1)))
    geographic_score = min(0.9, max(0.1, abs(latitude) / 90 + random.uniform(-0.1, 0.1)))
    economic_score = min(0.9, max(0.1, (current_month / 12) + random.uniform(-0.1, 0.1)))
    final_score = (climate_score + geographic_score + economic_score) / 3
    
    # æ„å»ºSHAPæ•°æ®
    shap_data = {
        'climate_score': round(climate_score, 3),
        'geographic_score': round(geographic_score, 3),
        'economic_score': round(economic_score, 3),
        'final_score': round(final_score, 3),
        'city': location_name,
        'overall_confidence': min(0.95, max(0.6, final_score + random.uniform(-0.1, 0.1))),
        'shap_analysis': {
            'feature_importance': {
                f'temperature_trend_{image_id}': round(random.uniform(0.05, 0.2), 3),
                f'humidity_factor_{image_id}': round(random.uniform(0.03, 0.15), 3),
                f'geographic_position_{image_id}': round(random.uniform(0.08, 0.18), 3),
                f'pressure_variation_{image_id}': round(random.uniform(0.02, 0.12), 3)
            }
        }
    }
    
    # ç”ŸæˆAIæ•…äº‹
    ai_story = generate_ai_environmental_story(shap_data)
    
    return {
        "id": image_id,
        "input_data": environmental_data,
        "result_data": {
            "temperature": round(15 + (image_id * 3.2) % 20, 1),  # æ¨¡æ‹Ÿæ¸©åº¦
            "humidity": round(40 + (image_id * 2.7) % 40, 1),     # æ¨¡æ‹Ÿæ¹¿åº¦
            "confidence": shap_data['overall_confidence'],
            "climate_type": _determine_climate_type(shap_data),
            "vegetation_index": _calculate_vegetation_index(shap_data),
            "predictions": {
                "short_term": _generate_short_term_prediction(shap_data),
                "long_term": _generate_long_term_prediction(shap_data)
            },
            
            # å®Œæ•´SHAPåˆ†æ
            "climate_score": shap_data['climate_score'],
            "geographic_score": shap_data['geographic_score'],
            "economic_score": shap_data['economic_score'],
            "final_score": shap_data['final_score'],
            "city": location_name,
            "shap_analysis": shap_data['shap_analysis'],
            "ai_story": ai_story,
            
            # åˆ†æå…ƒæ•°æ®
            "analysis_metadata": {
                "generated_at": datetime.now().isoformat(),
                "model_version": "fallback_dynamic_v1.0.0",
                "api_source": "fallback_generation",
                "location": location_name,
                "image_id": image_id,
                "ml_models_used": ["fallback_simulation"],
                "coordinates_source": "user_input" if latitude != 51.5074 or longitude != -0.1278 else "default"
            }
        },
        "prompt": f"Dynamic environmental analysis for {location_name} based on telescope observation #{image_id}",
        "location": location_name
    }

@images_bp.route('/<int:image_id>/refresh-story', methods=['POST'])
def refresh_image_story(image_id):
    """
    ğŸ”„ å¼ºåˆ¶é‡æ–°ç”ŸæˆæŒ‡å®šå›¾ç‰‡çš„AIæ•…äº‹
    """
    try:
        logger.info(f"ğŸ”„ Forcing story refresh for image {image_id}")
        
        # 1. åˆ é™¤æ•°æ®åº“ä¸­çš„ç¼“å­˜è®°å½•
        try:
            conn = psycopg2.connect(os.environ['DATABASE_URL'])
            cur = conn.cursor()
            
            # åˆ é™¤æ—§çš„åˆ†æè®°å½•
            cur.execute("DELETE FROM image_analysis WHERE image_id = %s", (image_id,))
            deleted_count = cur.rowcount
            
            conn.commit()
            cur.close()
            conn.close()
            
            logger.info(f"âœ… Deleted {deleted_count} cached analysis records for image {image_id}")
            
        except Exception as db_error:
            logger.warning(f"âš ï¸ Database cleanup failed: {db_error}")
        
        # 2. å¼ºåˆ¶ç”Ÿæˆæ–°çš„åˆ†æå’Œæ•…äº‹
        analysis_result = process_image_analysis(
            image_id=image_id,
            image_url=f"placeholder_url_{image_id}",
            description=f"Force refresh analysis for image {image_id}",
            prediction_id=None
        )
        
        if analysis_result and analysis_result.get('status') == 'completed':
            return jsonify({
                "success": True,
                "message": f"Story refreshed successfully for image {image_id}",
                "data": {
                    "image_id": image_id,
                    "new_story": analysis_result.get('ai_story', 'No story generated'),
                    "generated_at": analysis_result.get('generated_at'),
                    "refresh_timestamp": datetime.now().isoformat()
                }
            })
        else:
            return jsonify({
                "success": False,
                "error": "Failed to generate new analysis"
            }), 500
            
    except Exception as e:
        logger.error(f"âŒ Error refreshing story for image {image_id}: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@images_bp.route('/refresh-all-stories', methods=['POST'])
def refresh_all_stories():
    """
    ğŸ”„ å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡çš„AIæ•…äº‹ï¼ˆå±é™©æ“ä½œï¼Œä»…é™ç®¡ç†å‘˜ï¼‰
    """
    try:
        logger.info("ğŸ”„ Starting bulk story refresh operation")
        
        # è·å–æ‰€æœ‰å›¾ç‰‡ID
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
        cur.execute("DELETE FROM image_analysis")
        deleted_count = cur.rowcount
        
        # è·å–æ‰€æœ‰å›¾ç‰‡ID
        cur.execute("SELECT id FROM images ORDER BY id")
        image_ids = [row[0] for row in cur.fetchall()]
        
        conn.commit()
        cur.close()
        conn.close()
        
        logger.info(f"âœ… Cleared {deleted_count} cached records, found {len(image_ids)} images to refresh")
        
        return jsonify({
            "success": True,
            "message": f"Cleared all cached stories for {len(image_ids)} images",
            "data": {
                "cleared_cache_count": deleted_count,
                "total_images": len(image_ids),
                "refresh_timestamp": datetime.now().isoformat(),
                "note": "Stories will be regenerated when images are next viewed"
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ Error in bulk story refresh: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500
