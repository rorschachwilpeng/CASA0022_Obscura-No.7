#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Images API Routes - å›¾ç‰‡ä¸Šä¼ ä¸ç®¡ç†APIç«¯ç‚¹
"""

from flask import Blueprint, request, jsonify
import cloudinary.uploader
import psycopg2
import os
import logging
from datetime import datetime
from werkzeug.datastructures import FileStorage
import io
import json

# æ·»åŠ OpenAIå¯¼å…¥
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# æœ¬åœ°å›¾ç‰‡å­˜å‚¨ï¼ˆç”¨äºå¼€å‘ç¯å¢ƒï¼‰
LOCAL_IMAGES_STORE = {}

# æœ¬åœ°å¼€å‘æ—¶çš„åˆ†æç»“æœå­˜å‚¨
LOCAL_ANALYSIS_STORE = {}

logger = logging.getLogger(__name__)

# åˆ›å»ºè“å›¾
images_bp = Blueprint('images', __name__, url_prefix='/api/v1/images')

def process_image_analysis(image_id, image_url, description, prediction_id):
    """
    åœ¨å›¾ç‰‡ä¸Šä¼ åç«‹å³è¿›è¡Œåˆ†æå’Œæ•…äº‹ç”Ÿæˆ
    
    Args:
        image_id: å›¾ç‰‡ID
        image_url: å›¾ç‰‡URL
        description: å›¾ç‰‡æè¿°
        prediction_id: é¢„æµ‹ID
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    try:
        logger.info(f"ğŸ”„ Starting analysis for image {image_id}")
        
        # 1. ç”ŸæˆSHAPåˆ†ææ•°æ®
        shap_data = generate_shap_analysis_data(image_id, description)
        
        # 2. ç”ŸæˆAIæ•…äº‹
        story_data = generate_ai_environmental_story(shap_data)
        
        # 3. ç»„åˆåˆ†æç»“æœ
        analysis_result = {
            'image_id': image_id,
            'shap_analysis': shap_data,
            'ai_story': story_data,
            'generated_at': datetime.now().isoformat(),
            'status': 'completed'
        }
        
        # 4. å­˜å‚¨åˆ†æç»“æœ
        try:
            # å°è¯•å­˜å‚¨åˆ°æ•°æ®åº“
            conn = psycopg2.connect(os.environ['DATABASE_URL'])
            cur = conn.cursor()
            
            cur.execute("""
                INSERT INTO image_analysis (image_id, shap_data, ai_story, generated_at) 
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (image_id) DO UPDATE SET
                    shap_data = EXCLUDED.shap_data,
                    ai_story = EXCLUDED.ai_story,
                    generated_at = EXCLUDED.generated_at
            """, (image_id, json.dumps(shap_data), json.dumps(story_data), datetime.now()))
            
            conn.commit()
            cur.close()
            conn.close()
            
            logger.info(f"âœ… Analysis stored in database for image {image_id}")
            
        except Exception as db_error:
            logger.error(f"Database storage failed: {db_error}")
            # å­˜å‚¨åˆ°æœ¬åœ°
            LOCAL_ANALYSIS_STORE[image_id] = analysis_result
            logger.info(f"âœ… Analysis stored locally for image {image_id}")
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"âŒ Error processing image analysis: {e}")
        return {
            'image_id': image_id,
            'error': str(e),
            'generated_at': datetime.now().isoformat(),
            'status': 'failed'
        }

def generate_shap_analysis_data(image_id, description):
    """
    ç”ŸæˆSHAPåˆ†ææ•°æ®
    
    Args:
        image_id: å›¾ç‰‡ID
        description: å›¾ç‰‡æè¿°
        
    Returns:
        dict: SHAPåˆ†ææ•°æ®
    """
    # åŸºäºå›¾ç‰‡æè¿°ç”Ÿæˆæ¨¡æ‹Ÿçš„SHAPæ•°æ®
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„æœºå™¨å­¦ä¹ æ¨¡å‹
    
    # å†å²åŸºå‡†å€¼ï¼ˆæ¨¡æ‹Ÿå†å²æ•°æ®çš„å¹³å‡å€¼ï¼‰
    historical_baselines = {
        'climate_baseline': 0.68,      # æ°”å€™ç»´åº¦å†å²å‡å€¼
        'geographic_baseline': 0.65,   # åœ°ç†ç»´åº¦å†å²å‡å€¼
        'economic_baseline': 0.63      # ç»æµç»´åº¦å†å²å‡å€¼
    }
    
    # å½“å‰é¢„æµ‹å€¼ï¼ˆåŸºäºæè¿°è°ƒæ•´ï¼‰
    current_scores = {
        'climate_score': 0.72,
        'geographic_score': 0.69,
        'economic_score': 0.66
    }
    
    # æ ¹æ®æè¿°è°ƒæ•´å½“å‰é¢„æµ‹å€¼
    description_lower = description.lower()
    if 'tree' in description_lower or 'forest' in description_lower:
        current_scores['climate_score'] += 0.05
        current_scores['geographic_score'] += 0.08
    elif 'urban' in description_lower or 'city' in description_lower:
        current_scores['economic_score'] += 0.10
        current_scores['geographic_score'] += 0.03
    elif 'ocean' in description_lower or 'sea' in description_lower:
        current_scores['climate_score'] += 0.08
        current_scores['geographic_score'] += 0.12
    
    # è®¡ç®—æ€»ä½“åˆ†æ•°
    output_score = (current_scores['climate_score'] + current_scores['geographic_score'] + current_scores['economic_score']) / 3
    
    # è®¡ç®—ç›¸å¯¹å˜åŒ–ç™¾åˆ†æ¯”ï¼ˆä½¿ç”¨ML_Modelsä¸­çš„æ­£ç¡®å…¬å¼ï¼‰
    def calculate_relative_change(current_value, baseline_value):
        """
        è®¡ç®—ç›¸å¯¹äºå†å²åŸºå‡†çš„å˜åŒ–ç™¾åˆ†æ¯”
        å…¬å¼: (å½“å‰å€¼ - å†å²å‡å€¼) / å†å²å‡å€¼ * 100%
        """
        if baseline_value == 0:
            return 0
        relative_change = ((current_value - baseline_value) / baseline_value) * 100
        return round(relative_change, 1)
    
    # ç”Ÿæˆæ­£è´Ÿå˜åŒ–æ•°æ®
    dimension_changes = {
        'climate_change': calculate_relative_change(
            current_scores['climate_score'], 
            historical_baselines['climate_baseline']
        ),
        'geographic_change': calculate_relative_change(
            current_scores['geographic_score'], 
            historical_baselines['geographic_baseline']
        ),
        'economic_change': calculate_relative_change(
            current_scores['economic_score'], 
            historical_baselines['economic_baseline']
        )
    }
    
    # ç”Ÿæˆå±‚æ¬¡åŒ–ç‰¹å¾é‡è¦æ€§
    hierarchical_features = {
        'climate': {
            'temperature_trend': 0.15,
            'precipitation_pattern': 0.12,
            'humidity_variation': 0.08,
            'seasonal_change': 0.10
        },
        'geographic': {
            'elevation_factor': 0.11,
            'terrain_complexity': 0.09,
            'vegetation_density': 0.13,
            'water_proximity': 0.07
        },
        'economic': {
            'development_index': 0.08,
            'infrastructure_score': 0.06,
            'resource_availability': 0.09,
            'population_density': 0.05
        }
    }
    
    # ç”Ÿæˆæ•°æ®éªŒè¯ç»“æœ
    data_validation = {
        'is_valid': True,
        'validation_score': 0.94,
        'errors': [],
        'warnings': ['Some features may have lower confidence due to limited data'],
        'data_quality_score': 0.91
    }
    
    return {
        'image_id': image_id,
        'output_score': output_score,
        'climate_score': current_scores['climate_score'],
        'geographic_score': current_scores['geographic_score'],
        'economic_score': current_scores['economic_score'],
        'climate_change': dimension_changes['climate_change'],
        'geographic_change': dimension_changes['geographic_change'],
        'economic_change': dimension_changes['economic_change'],
        'climate_baseline': historical_baselines['climate_baseline'],
        'geographic_baseline': historical_baselines['geographic_baseline'],
        'economic_baseline': historical_baselines['economic_baseline'],
        'hierarchical_features': hierarchical_features,
        'data_validation': data_validation,
        'generated_at': datetime.now().isoformat()
    }

def transform_to_hierarchical_shap_data(flat_shap_data):
    """
    å°†å¹³é¢çš„SHAPæ•°æ®è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„ï¼Œç”¨äºåœ†å½¢æ‰“åŒ…å›¾å¯è§†åŒ–
    
    Args:
        flat_shap_data: åŒ…å«flat feature_importanceçš„åŸå§‹SHAPæ•°æ®
        
    Returns:
        dict: å±‚æ¬¡åŒ–çš„SHAPæ•°æ®ç»“æ„
    """
    if not flat_shap_data or 'shap_analysis' not in flat_shap_data:
        return flat_shap_data
    
    original_features = flat_shap_data['shap_analysis'].get('feature_importance', {})
    
    # å®šä¹‰ç‰¹å¾åˆ°ç»´åº¦çš„æ˜ å°„
    feature_mapping = {
        'climate': {
            'temperature': original_features.get('temperature', 0.0),
            'humidity': original_features.get('humidity', 0.0),
            'climate_zone': original_features.get('climate_zone', 0.0),
            'precipitation': original_features.get('precipitation', 0.0),
            'wind_speed': original_features.get('wind_speed', 0.0)
        },
        'geographic': {
            'location_factor': original_features.get('location_factor', 0.0),
            'pressure': original_features.get('pressure', 0.0),
            'elevation': original_features.get('elevation', 0.0),
            'latitude': original_features.get('latitude', 0.0),
            'longitude': original_features.get('longitude', 0.0)
        },
        'economic': {
            'seasonal_factor': original_features.get('seasonal_factor', 0.0),
            'population_density': original_features.get('population_density', 0.0),
            'urban_index': original_features.get('urban_index', 0.0),
            'infrastructure': original_features.get('infrastructure', 0.0)
        }
    }
    
    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ€»é‡è¦æ€§
    dimension_scores = {}
    for dimension, features in feature_mapping.items():
        # è¿‡æ»¤æ‰å€¼ä¸º0çš„ç‰¹å¾
        active_features = {k: v for k, v in features.items() if v > 0}
        dimension_scores[dimension] = {
            'total_importance': sum(active_features.values()),
            'feature_count': len(active_features),
            'features': active_features
        }
    
    # åˆ›å»ºå¢å¼ºçš„SHAPæ•°æ®
    enhanced_shap_data = dict(flat_shap_data)
    enhanced_shap_data['shap_analysis']['hierarchical_features'] = {
        'climate': dimension_scores['climate'],
        'geographic': dimension_scores['geographic'], 
        'economic': dimension_scores['economic']
    }
    
    # æ·»åŠ åœ†å½¢æ‰“åŒ…å›¾æ‰€éœ€çš„æ•°æ®æ ¼å¼
    enhanced_shap_data['shap_analysis']['pack_chart_data'] = generate_pack_chart_data(
        dimension_scores, 
        flat_shap_data.get('final_score', 0.7)
    )
    
    return enhanced_shap_data

def generate_pack_chart_data(dimension_scores, final_score):
    """
    ç”Ÿæˆåœ†å½¢æ‰“åŒ…å›¾æ‰€éœ€çš„æ•°æ®æ ¼å¼
    
    Args:
        dimension_scores: ç»´åº¦å¾—åˆ†æ•°æ®
        final_score: æœ€ç»ˆå¾—åˆ†
        
    Returns:
        dict: åœ†å½¢æ‰“åŒ…å›¾æ•°æ®ç»“æ„
    """
    pack_data = {
        "name": "Environmental Impact",
        "value": final_score,
        "children": []
    }
    
    # ç»´åº¦é¢œè‰²æ˜ å°„ï¼ˆè’¸æ±½æœ‹å…‹ä¸»é¢˜ï¼‰
    dimension_colors = {
        'climate': '#d4af37',      # é‡‘è‰²
        'geographic': '#cd853f',    # ç§˜é²è‰²
        'economic': '#8b4513'       # é©¬éæ£•è‰²
    }
    
    for dimension, data in dimension_scores.items():
        if data['total_importance'] > 0:
            dimension_node = {
                "name": dimension.title(),
                "value": data['total_importance'],
                "itemStyle": {"color": dimension_colors.get(dimension, '#888888')},
                "children": []
            }
            
            # æ·»åŠ ç‰¹å¾èŠ‚ç‚¹
            for feature, importance in data['features'].items():
                if importance > 0:
                    feature_node = {
                        "name": feature.replace('_', ' ').title(),
                        "value": importance,
                        "itemStyle": {"color": dimension_colors.get(dimension, '#888888')},
                        "tooltip": {
                            "formatter": f"{feature.replace('_', ' ').title()}: {importance:.3f}"
                        }
                    }
                    dimension_node["children"].append(feature_node)
            
            pack_data["children"].append(dimension_node)
    
    return pack_data

def validate_hierarchical_shap_data(shap_data):
    """
    éªŒè¯å±‚æ¬¡åŒ–SHAPæ•°æ®çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§
    
    Args:
        shap_data: å±‚æ¬¡åŒ–çš„SHAPæ•°æ®
        
    Returns:
        dict: éªŒè¯ç»“æœï¼ŒåŒ…å«is_validå¸ƒå°”å€¼å’Œè¯¦ç»†æŠ¥å‘Š
    """
    validation_result = {
        "is_valid": True,
        "errors": [],
        "warnings": [],
        "completeness_report": {},
        "dimension_analysis": {}
    }
    
    # æ£€æŸ¥å¿…éœ€çš„é¡¶çº§å­—æ®µ
    required_fields = ['climate_score', 'geographic_score', 'economic_score', 'final_score']
    for field in required_fields:
        if field not in shap_data:
            validation_result["errors"].append(f"Missing required field: {field}")
            validation_result["is_valid"] = False
        elif not isinstance(shap_data[field], (int, float)):
            validation_result["errors"].append(f"Invalid type for {field}: expected number")
            validation_result["is_valid"] = False
    
    # æ£€æŸ¥SHAPåˆ†æç»“æ„
    if 'shap_analysis' not in shap_data:
        validation_result["errors"].append("Missing shap_analysis section")
        validation_result["is_valid"] = False
        return validation_result
    
    shap_section = shap_data['shap_analysis']
    
    # æ£€æŸ¥å±‚æ¬¡åŒ–ç‰¹å¾æ•°æ®
    if 'hierarchical_features' in shap_section:
        hierarchical = shap_section['hierarchical_features']
        required_dimensions = ['climate', 'geographic', 'economic']
        
        for dimension in required_dimensions:
            if dimension not in hierarchical:
                validation_result["errors"].append(f"Missing dimension: {dimension}")
                validation_result["is_valid"] = False
            else:
                dim_data = hierarchical[dimension]
                
                # éªŒè¯ç»´åº¦æ•°æ®ç»“æ„
                if 'total_importance' not in dim_data:
                    validation_result["warnings"].append(f"Missing total_importance for {dimension}")
                if 'features' not in dim_data:
                    validation_result["errors"].append(f"Missing features for {dimension}")
                    validation_result["is_valid"] = False
                else:
                    # ç»Ÿè®¡ç‰¹å¾æ•°é‡å’Œé‡è¦æ€§æ€»å’Œ
                    features = dim_data['features']
                    feature_count = len(features)
                    importance_sum = sum(features.values()) if features else 0
                    
                    validation_result["dimension_analysis"][dimension] = {
                        "feature_count": feature_count,
                        "importance_sum": importance_sum,
                        "features_list": list(features.keys())
                    }
                    
                    if feature_count == 0:
                        validation_result["warnings"].append(f"No active features for {dimension}")
    
    # æ£€æŸ¥åœ†å½¢æ‰“åŒ…å›¾æ•°æ®
    if 'pack_chart_data' in shap_section:
        pack_data = shap_section['pack_chart_data']
        if 'children' not in pack_data:
            validation_result["warnings"].append("Missing children in pack_chart_data")
        else:
            validation_result["completeness_report"]["pack_chart_children"] = len(pack_data['children'])
    
    # æ€»ä½“å®Œæ•´æ€§è¯„åˆ†
    total_features = sum(
        analysis.get('feature_count', 0) 
        for analysis in validation_result["dimension_analysis"].values()
    )
    validation_result["completeness_report"]["total_features"] = total_features
    validation_result["completeness_report"]["has_ai_story"] = 'ai_story' in shap_data
    
    return validation_result

def generate_ai_environmental_story(shap_data):
    """
    ä½¿ç”¨DeepSeekç”Ÿæˆç¯å¢ƒæ•…äº‹ï¼ˆçº¦100è¯è‹±æ–‡ï¼Œæˆå‰§æ€§æè¿°ï¼‰
    
    Args:
        shap_data: SHAPåˆ†ææ•°æ®ï¼ŒåŒ…å«ä¸‰ä¸ªç»´åº¦å¾—åˆ†å’Œç‰¹å¾é‡è¦æ€§
        
    Returns:
        str: ç”Ÿæˆçš„è‹±æ–‡ç¯å¢ƒæ•…äº‹
    """
    # è·å–DeepSeek APIå¯†é’¥
    deepseek_key = os.getenv('DEEPSEEK_API_KEY')
    if not deepseek_key:
        logger.warning("DeepSeek API key not found, using fallback story")
        return generate_fallback_story(shap_data)
    
    try:
        import requests
        
        # æ„å»ºç”¨äºæ•…äº‹ç”Ÿæˆçš„prompt
        climate_score = shap_data.get('climate_score', 0.5) * 100
        geographic_score = shap_data.get('geographic_score', 0.5) * 100  
        economic_score = shap_data.get('economic_score', 0.5) * 100
        city = shap_data.get('city', 'Unknown Location')
        
        # è·å–ä¸»è¦ç‰¹å¾å½±å“
        feature_importance = shap_data.get('shap_analysis', {}).get('feature_importance', {})
        top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:3]
        
        prompt = f"""Write a dramatic environmental narrative in exactly 100 words. 

Location: {city}
Climate Impact: {climate_score:.1f}%
Geographic Impact: {geographic_score:.1f}% 
Economic Impact: {economic_score:.1f}%
Key factors: {', '.join([f[0] for f in top_features[:3]])}

Create a compelling story that dramatically describes the environmental conditions and future predictions for this location. Use vivid imagery and emotional language. Focus on the interplay between climate, geography, and economics. Make it sound like a scene from a climate science thriller.

Write EXACTLY 100 words. Be dramatic and engaging."""

        # è°ƒç”¨DeepSeek API
        headers = {
            'Authorization': f'Bearer {deepseek_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": "You are an environmental storyteller who creates dramatic narratives based on scientific data."},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 150,
            "temperature": 0.8
        }
        
        response = requests.post(
            'https://api.deepseek.com/v1/chat/completions',
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            story = result['choices'][0]['message']['content'].strip()
            logger.info(f"âœ… DeepSeek AI story generated successfully for {city}")
            return story
        else:
            logger.error(f"âŒ DeepSeek API error: {response.status_code} - {response.text}")
            return generate_fallback_story(shap_data)
        
    except Exception as e:
        logger.error(f"âŒ DeepSeek story generation failed: {e}")
        return generate_fallback_story(shap_data)

def generate_fallback_story(shap_data):
    """
    ç”Ÿæˆå¤‡ç”¨æ•…äº‹ï¼ˆå½“OpenAIä¸å¯ç”¨æ—¶ï¼‰
    """
    climate_score = shap_data.get('climate_score', 0.5) * 100
    geographic_score = shap_data.get('geographic_score', 0.5) * 100
    economic_score = shap_data.get('economic_score', 0.5) * 100
    city = shap_data.get('city', 'Unknown Location')
    
    # åŸºäºå¾—åˆ†ç”Ÿæˆä¸åŒçš„æ•…äº‹æ¨¡æ¿
    if climate_score > 70:
        climate_desc = "thriving under stable atmospheric conditions"
    elif climate_score > 50:
        climate_desc = "adapting to moderate environmental pressures"
    else:
        climate_desc = "struggling against challenging climatic forces"
        
    if geographic_score > 70:
        geo_desc = "blessed with favorable topographical features"
    elif geographic_score > 50:
        geo_desc = "shaped by diverse geographical influences"
    else:
        geo_desc = "constrained by complex terrain challenges"
        
    if economic_score > 70:
        econ_desc = "supported by robust economic foundations"
    elif economic_score > 50:
        econ_desc = "balanced between growth and sustainability"
    else:
        econ_desc = "facing economic transformation pressures"
    
    story = f"""In {city}, an intricate environmental drama unfolds. The ecosystem stands {climate_desc}, while being {geo_desc}. The region remains {econ_desc}. Through SHAP analysis, we witness nature's delicate balance - where climate forces ({climate_score:.1f}%), geographic patterns ({geographic_score:.1f}%), and economic dynamics ({economic_score:.1f}%) converge to shape tomorrow's environmental narrative. This location tells a story of resilience, adaptation, and the profound interconnectedness of our planet's complex systems."""
    
    logger.info(f"âœ… Fallback story generated for {city}")
    return story

@images_bp.route('', methods=['POST'])
def upload_image():
    """
    å›¾ç‰‡ä¸Šä¼ APIç«¯ç‚¹
    
    æ¥æ”¶: multipart/form-data åŒ…å« file, description, prediction_id
    è¿”å›: å›¾ç‰‡ä¿¡æ¯JSON
    """
    try:
        # æ·»åŠ è°ƒè¯•ä»£ç  - å¼€å§‹
        logger.info("=== DEBUG: Upload request received ===")
        logger.info(f"Request content type: {request.content_type}")
        logger.info(f"Request files keys: {list(request.files.keys())}")
        logger.info(f"Request form keys: {list(request.form.keys())}")
        
        # æ£€æŸ¥åŸå§‹æ•°æ®
        logger.info(f"Request data length: {len(request.data)}")
        logger.info(f"Request stream available: {hasattr(request, 'stream')}")
        
        # å°è¯•æ‰‹åŠ¨è§£ææ–‡ä»¶ - å¦‚æœFlaskè§£æå¤±è´¥
        file = None
        description = request.form.get('description', '')
        prediction_id = request.form.get('prediction_id')
        
        # æ–¹æ³•1: æ ‡å‡†Flaskæ–¹å¼
        if 'file' in request.files:
            file = request.files['file']
            logger.info(f"Found file via Flask: {file.filename}")
        
        # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦æœ‰Noneé”®ï¼ˆè¯´æ˜è§£æå‡ºé”™ï¼‰
        elif None in request.form:
            # æ–‡ä»¶å†…å®¹è¢«å½“ä½œè¡¨å•æ•°æ®äº†ï¼Œå°è¯•é‡å»ºFileStorageå¯¹è±¡
            file_content = request.form[None]
            if file_content and file_content.startswith(b'\xff\xd8\xff') or file_content.startswith('JFIF'):
                # è¿™æ˜¯JPEGæ–‡ä»¶å†…å®¹
                file_bytes = file_content.encode('latin-1') if isinstance(file_content, str) else file_content
                file = FileStorage(
                    stream=io.BytesIO(file_bytes),
                    filename="uploaded_image.jpg",
                    content_type="image/jpeg"
                )
                logger.info("Reconstructed file from form data")
        
        # è°ƒè¯•ä»£ç  - ç»“æŸ
        logger.info(f"Final extracted - file: {file}, description: {description}, prediction_id: {prediction_id}")
        
        if not file or file.filename == '':
            return jsonify({
                "success": False,
                "error": "No file provided",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        # éªŒè¯æ–‡ä»¶ç±»å‹
        allowed_extensions = {'png', 'jpg', 'jpeg', 'gif', 'webp'}
        file_ext = file.filename.rsplit('.', 1)[1].lower() if '.' in file.filename else ''
        
        if file_ext not in allowed_extensions:
            return jsonify({
                "success": False,
                "error": f"Invalid file type. Allowed: {', '.join(allowed_extensions)}",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        # ä¸Šä¼ å›¾ç‰‡åˆ°Cloudinary
        try:
            upload_result = cloudinary.uploader.upload(
                file,
                folder="obscura_images",
                use_filename=True,
                unique_filename=True
            )
            
            image_url = upload_result['secure_url']
            thumbnail_url = upload_result.get('secure_url', image_url)
            
            logger.info(f"Image uploaded to Cloudinary: {image_url}")
            
        except Exception as e:
            logger.error(f"Cloudinary upload failed: {e}")
            return jsonify({
                "success": False,
                "error": f"Image upload failed: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }), 500
        
        # æœ¬åœ°å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨æœ¬åœ°å­˜å‚¨ä»£æ›¿æ•°æ®åº“
        database_url = os.getenv("DATABASE_URL")
        if not database_url or "nodename nor servname provided" in str(database_url):
            logger.info("Using local storage for development environment")
            
            # ç”Ÿæˆæœ¬åœ°å›¾ç‰‡ID
            image_id = len(LOCAL_IMAGES_STORE) + 1
            created_at = datetime.now()
            
            # å­˜å‚¨åˆ°æœ¬åœ°
            LOCAL_IMAGES_STORE[image_id] = {
                'id': image_id,
                'url': image_url,
                'thumbnail_url': thumbnail_url,
                'description': description,
                'prediction_id': int(prediction_id) if prediction_id else 1,
                'created_at': created_at
            }
            
            logger.info(f"Image stored locally with ID: {image_id}")
            
            return jsonify({
                "success": True,
                "image": {
                    "id": image_id,
                    "url": image_url,
                    "thumbnail_url": thumbnail_url,
                    "description": description,
                    "prediction_id": int(prediction_id) if prediction_id else 1,
                    "created_at": created_at.isoformat()
                },
                "message": "Image uploaded successfully (local dev mode)",
                "timestamp": datetime.now().isoformat()
            }), 201
        
        # ä¿å­˜å›¾ç‰‡ä¿¡æ¯åˆ°æ•°æ®åº“ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
        try:
            conn = psycopg2.connect(os.environ['DATABASE_URL'])
            cur = conn.cursor()
            
            # è‡ªåŠ¨åˆ›å»ºpredictionè®°å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            prediction_id_int = int(prediction_id) if prediction_id else 1
            
            # æ£€æŸ¥predictionè®°å½•æ˜¯å¦å­˜åœ¨
            cur.execute("SELECT id FROM predictions WHERE id = %s", (prediction_id_int,))
            existing_prediction = cur.fetchone()
            
            if not existing_prediction:
                logger.info(f"Creating missing prediction record with ID: {prediction_id_int}")
                
                # åˆ›å»ºé»˜è®¤çš„predictionè®°å½•
                cur.execute("""
                    INSERT INTO predictions (
                        id, temperature, humidity, pressure, wind_speed, 
                        coordinates, weather_description, created_at, model_confidence
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s
                    ) ON CONFLICT (id) DO NOTHING
                """, (
                    prediction_id_int,
                    15.0,  # é»˜è®¤æ¸©åº¦
                    60.0,  # é»˜è®¤æ¹¿åº¦
                    1013.0,  # é»˜è®¤æ°”å‹
                    5.0,   # é»˜è®¤é£é€Ÿ
                    '{"latitude": 51.5074, "longitude": -0.1278}',  # London coordinates
                    'System generated prediction for image upload',
                    datetime.now(),
                    1.0    # é»˜è®¤ç½®ä¿¡åº¦
                ))
                logger.info(f"âœ… Default prediction record created with ID: {prediction_id_int}")
            
            # ç°åœ¨æ’å…¥imageè®°å½•
            cur.execute("""
                INSERT INTO images (url, thumbnail_url, description, prediction_id, created_at) 
                VALUES (%s, %s, %s, %s, %s) 
                RETURNING id, created_at
            """, (image_url, thumbnail_url, description, prediction_id_int, datetime.now()))
            
            result = cur.fetchone()
            image_id, created_at = result
            
            conn.commit()
            cur.close()
            conn.close()
            
            logger.info(f"Image record saved to database with ID: {image_id}")
            
        except Exception as e:
            logger.error(f"Database insert failed: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“è¿æ¥é—®é¢˜
            if "nodename nor servname provided" in str(e) or "could not translate host name" in str(e):
                logger.info("Database unavailable - creating local record for development")
                
                # ç”Ÿæˆæœ¬åœ°ID
                new_image_id = max(LOCAL_IMAGES_STORE.keys()) + 1 if LOCAL_IMAGES_STORE else 1
                
                # åˆ›å»ºæœ¬åœ°è®°å½•
                LOCAL_IMAGES_STORE[new_image_id] = {
                    'id': new_image_id,
                    'url': image_url,
                    'thumbnail_url': thumbnail_url,
                    'description': description,
                    'prediction_id': int(prediction_id),
                    'created_at': datetime.now(),
                    'location': 'Local Upload Test'
                }
                
                logger.info(f"Image stored locally with ID: {new_image_id}")
                
                # å¯åŠ¨åå°åˆ†æä»»åŠ¡
                import threading
                
                def run_analysis():
                    try:
                        # è¿è¡Œåˆ†æ
                        result = process_image_analysis(new_image_id, image_url, description, int(prediction_id))
                        logger.info(f"ğŸ“Š Analysis completed for image {new_image_id}: {result['status']}")
                    except Exception as e:
                        logger.error(f"âŒ Background analysis failed: {e}")
                
                # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œåˆ†æ
                analysis_thread = threading.Thread(target=run_analysis)
                analysis_thread.daemon = True
                analysis_thread.start()
                
                return jsonify({
                    "success": True,
                    "image": {
                        "id": new_image_id,
                        "url": image_url,
                        "thumbnail_url": thumbnail_url,
                        "description": description,
                        "prediction_id": int(prediction_id),
                        "created_at": datetime.now().isoformat()
                    },
                    "message": "Image uploaded successfully (local development mode)",
                    "analysis_status": "processing",
                    "timestamp": datetime.now().isoformat()
                }), 201
            else:
                return jsonify({
                    "success": False,
                    "error": f"Database save failed: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }), 500
        
        # å¯åŠ¨åå°åˆ†æä»»åŠ¡ï¼ˆæ•°æ®åº“æ¨¡å¼ï¼‰
        import threading
        
        def run_analysis():
            try:
                # è¿è¡Œåˆ†æ
                result = process_image_analysis(image_id, image_url, description, int(prediction_id))
                logger.info(f"ğŸ“Š Analysis completed for image {image_id}: {result['status']}")
            except Exception as e:
                logger.error(f"âŒ Background analysis failed: {e}")
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œåˆ†æ
        analysis_thread = threading.Thread(target=run_analysis)
        analysis_thread.daemon = True
        analysis_thread.start()
        
        # è¿”å›æˆåŠŸå“åº”
        return jsonify({
            "success": True,
            "image": {
                "id": image_id,
                "url": image_url,
                "thumbnail_url": thumbnail_url,
                "description": description,
                "prediction_id": int(prediction_id),
                "created_at": created_at.isoformat()
            },
            "message": "Image uploaded successfully",
            "analysis_status": "processing",
            "timestamp": datetime.now().isoformat()
        }), 201
        
    except Exception as e:
        logger.error(f"Unexpected error in upload_image: {e}")
        return jsonify({
            "success": False,
            "error": "Internal server error",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/register', methods=['POST'])
def register_image():
    """
    å›¾ç‰‡URLæ³¨å†ŒAPIç«¯ç‚¹ - ä¸“é—¨ç”¨äºæ³¨å†Œå·²ä¸Šä¼ åˆ°Cloudinaryçš„å›¾ç‰‡
    
    æ¥æ”¶: JSONæ ¼å¼çš„å›¾ç‰‡URLå’Œå…ƒæ•°æ®
    è¿”å›: å›¾ç‰‡ä¿¡æ¯JSON
    """
    try:
        # éªŒè¯è¯·æ±‚æ ¼å¼
        if not request.is_json:
            return jsonify({
                "success": False,
                "error": "Request must be JSON format",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        data = request.get_json()
        if not data:
            return jsonify({
                "success": False,
                "error": "No JSON data provided",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        # éªŒè¯å¿…è¦å‚æ•°
        if 'url' not in data:
            return jsonify({
                "success": False,
                "error": "Missing required parameter: url",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        # æå–å‚æ•°
        image_url = data['url']
        description = data.get('description', 'Telescope generated artwork')
        source = data.get('source', 'cloudinary_telescope')
        metadata = data.get('metadata', {})
        
        # é»˜è®¤prediction_id - å¯ä»¥ä»metadataä¸­æå–æˆ–ä½¿ç”¨é»˜è®¤å€¼
        prediction_id = 1  # é»˜è®¤é¢„æµ‹ID
        if metadata and 'style' in metadata and 'prediction_id' in metadata['style']:
            try:
                prediction_id = int(metadata['style']['prediction_id'])
            except (ValueError, TypeError):
                prediction_id = 1
        
        # ç”Ÿæˆç¼©ç•¥å›¾URLï¼ˆCloudinaryè‡ªåŠ¨ç”Ÿæˆï¼‰
        thumbnail_url = image_url
        
        logger.info(f"Registering Cloudinary image: {image_url}")
        
        # ä¿å­˜å›¾ç‰‡ä¿¡æ¯åˆ°æ•°æ®åº“
        try:
            conn = psycopg2.connect(os.environ['DATABASE_URL'])
            cur = conn.cursor()
            
            # è‡ªåŠ¨åˆ›å»ºpredictionè®°å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            # æ£€æŸ¥predictionè®°å½•æ˜¯å¦å­˜åœ¨
            cur.execute("SELECT id FROM predictions WHERE id = %s", (prediction_id,))
            existing_prediction = cur.fetchone()
            
            if not existing_prediction:
                logger.info(f"Creating missing prediction record with ID: {prediction_id}")
                
                # åˆ›å»ºé»˜è®¤çš„predictionè®°å½•
                cur.execute("""
                    INSERT INTO predictions (
                        id, temperature, humidity, pressure, wind_speed, 
                        coordinates, weather_description, created_at, model_confidence
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s
                    ) ON CONFLICT (id) DO NOTHING
                """, (
                    prediction_id,
                    15.0,  # é»˜è®¤æ¸©åº¦
                    60.0,  # é»˜è®¤æ¹¿åº¦
                    1013.0,  # é»˜è®¤æ°”å‹
                    5.0,   # é»˜è®¤é£é€Ÿ
                    '{"latitude": 51.5074, "longitude": -0.1278}',  # London coordinates
                    'System generated prediction for image registration',
                    datetime.now(),
                    1.0    # é»˜è®¤ç½®ä¿¡åº¦
                ))
                logger.info(f"âœ… Default prediction record created with ID: {prediction_id}")
            
            # ç°åœ¨æ’å…¥imageè®°å½•
            cur.execute("""
                INSERT INTO images (url, thumbnail_url, description, prediction_id, created_at) 
                VALUES (%s, %s, %s, %s, %s) 
                RETURNING id, created_at
            """, (image_url, thumbnail_url, description, prediction_id, datetime.now()))
            
            result = cur.fetchone()
            image_id, created_at = result
            
            conn.commit()
            cur.close()
            conn.close()
            
            logger.info(f"Image registered in database with ID: {image_id}")
            
        except Exception as e:
            logger.error(f"Database insert failed: {e}")
            return jsonify({
                "success": False,
                "error": f"Database save failed: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }), 500
        
        # è¿”å›æˆåŠŸå“åº”
        return jsonify({
            "success": True,
            "image": {
                "id": image_id,
                "url": image_url,
                "thumbnail_url": thumbnail_url,
                "description": description,
                "prediction_id": prediction_id,
                "created_at": created_at.isoformat(),
                "source": source
            },
            "message": "Image registered successfully",
            "timestamp": datetime.now().isoformat()
        }), 201
        
    except Exception as e:
        logger.error(f"Unexpected error in register_image: {e}")
        return jsonify({
            "success": False,
            "error": "Internal server error",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('', methods=['GET'])
def get_images():
    """
    è·å–å›¾ç‰‡åˆ—è¡¨APIç«¯ç‚¹
    
    è¿”å›: æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯çš„JSONåˆ—è¡¨
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # æŸ¥è¯¢æ‰€æœ‰å›¾ç‰‡ï¼ŒæŒ‰åˆ›å»ºæ—¶é—´å€’åº
        cur.execute("""
            SELECT id, url, thumbnail_url, description, prediction_id, created_at 
            FROM images 
            ORDER BY created_at DESC
        """)
        
        images = []
        for row in cur.fetchall():
            images.append({
                "id": row[0],
                "url": row[1],
                "thumbnail_url": row[2],
                "description": row[3],
                "prediction_id": row[4],
                "created_at": row[5].isoformat()
            })
        
        cur.close()
        conn.close()
        
        logger.info(f"Retrieved {len(images)} images from database")
        
        return jsonify({
            "success": True,
            "images": images,
            "count": len(images),
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error fetching images: {e}")
        
        # æœ¬åœ°å¼€å‘æ¨¡å¼ï¼šå½“æ•°æ®åº“ä¸å¯ç”¨æ—¶ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
        if "nodename nor servname provided" in str(e) or "could not translate host name" in str(e):
            logger.info("Database unavailable - returning mock data for local development")
            
            # æ¨¡æ‹Ÿå›¾ç‰‡æ•°æ®
            mock_images = [
                {
                    "id": 1,
                    "url": "https://res.cloudinary.com/dvbgtqko/image/upload/v1750191245/obscura_images/file_lksfai.png",
                    "thumbnail_url": "https://res.cloudinary.com/dvbgtqko/image/upload/v1750191245/obscura_images/file_lksfai.png",
                    "description": "AI Generated Environmental Vision",
                    "prediction_id": 2,
                    "created_at": datetime.now().isoformat()
                },
                {
                    "id": 2,
                    "url": "https://res.cloudinary.com/dvbgtqko/image/upload/v1750191245/obscura_images/file_lksfai.png",
                    "thumbnail_url": "https://res.cloudinary.com/dvbgtqko/image/upload/v1750191245/obscura_images/file_lksfai.png",
                    "description": "Environmental Prediction Analysis",
                    "prediction_id": 3,
                    "created_at": datetime.now().isoformat()
                },
                {
                    "id": 3,
                    "url": "https://res.cloudinary.com/dvbgtqko/image/upload/v1750191245/obscura_images/file_lksfai.png", 
                    "thumbnail_url": "https://res.cloudinary.com/dvbgtqko/image/upload/v1750191245/obscura_images/file_lksfai.png",
                    "description": "Climate Change Visualization",
                    "prediction_id": 4,
                    "created_at": datetime.now().isoformat()
                }
            ]
            
            return jsonify({
                "success": True,
                "images": mock_images,
                "count": len(mock_images),
                "timestamp": datetime.now().isoformat(),
                "mode": "mock_data_for_local_development"
            })
        
        # å…¶ä»–æ•°æ®åº“é”™è¯¯
        return jsonify({
            "success": False,
            "error": "Failed to fetch images",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/<int:image_id>', methods=['GET'])
def get_image_detail(image_id):
    """
    è·å–å•å¼ å›¾ç‰‡è¯¦æƒ…APIç«¯ç‚¹
    
    è¿”å›: å›¾ç‰‡ä¿¡æ¯åŠå…³è”çš„é¢„æµ‹æ•°æ®
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # è”æŸ¥å›¾ç‰‡å’Œé¢„æµ‹æ•°æ®
        cur.execute("""
            SELECT 
                i.id, i.url, i.thumbnail_url, i.description, i.created_at,
                p.id as prediction_id, p.input_data, p.result_data, p.prompt, p.location
            FROM images i
            LEFT JOIN predictions p ON i.prediction_id = p.id
            WHERE i.id = %s
        """, (image_id,))
        
        row = cur.fetchone()
        if not row:
            return jsonify({
                "success": False,
                "error": "Image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        image_detail = {
            "id": row[0],
            "url": row[1],
            "thumbnail_url": row[2],
            "description": row[3],
            "created_at": row[4].isoformat(),
            "prediction": {
                "id": row[5],
                "input_data": row[6],
                "result_data": row[7],
                "prompt": row[8],
                "location": row[9]
            } if row[5] else None
        }
        
        cur.close()
        conn.close()
        
        return jsonify({
            "success": True,
            "image": image_detail,
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error fetching image detail: {e}")
        
        # æœ¬åœ°å¼€å‘æ¨¡å¼ï¼šå½“æ•°æ®åº“ä¸å¯ç”¨æ—¶ï¼Œæ£€æŸ¥æœ¬åœ°å­˜å‚¨æˆ–è¿”å›æ¨¡æ‹Ÿæ•°æ®
        if "nodename nor servname provided" in str(e) or "could not translate host name" in str(e):
            logger.info(f"Database unavailable - checking local storage for image {image_id}")
            
            # é¦–å…ˆæ£€æŸ¥æœ¬åœ°å­˜å‚¨
            if image_id in LOCAL_IMAGES_STORE:
                local_image = LOCAL_IMAGES_STORE[image_id]
                logger.info(f"Found image in local storage: {local_image['url']}")
                
                return jsonify({
                    "success": True,
                    "image": {
                        "id": local_image['id'],
                        "url": local_image['url'],
                        "thumbnail_url": local_image.get('thumbnail_url', local_image['url']),
                        "description": local_image['description'],
                        "created_at": local_image['created_at'].isoformat(),
                        "prediction": {
                            "id": local_image.get('prediction_id', 1),
                            "input_data": {
                                "temperature": 22.5,
                                "humidity": 65.0,
                                "location": local_image.get('location', 'Local Upload'),
                                "timestamp": local_image['created_at'].isoformat()
                            },
                            "result_data": {
                                "temperature": 23.8,
                                "humidity": 68.2,
                                "confidence": 0.87,
                                "climate_type": "temperate",
                                "vegetation_index": 0.73,
                                "predictions": {
                                    "short_term": "Moderate warming expected",
                                    "long_term": "Stable climate conditions"
                                }
                            },
                            "prompt": "Generate environmental vision based on current climate data",
                            "location": local_image.get('location', 'Local Upload')
                        }
                    },
                    "timestamp": datetime.now().isoformat(),
                    "mode": "local_storage"
                }), 200
            
            # å¦‚æœæœ¬åœ°å­˜å‚¨ä¸­æ²¡æœ‰ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
            logger.info(f"Image {image_id} not found in local storage - returning mock data")
            
            # æ¨¡æ‹Ÿå›¾ç‰‡è¯¦æƒ…æ•°æ®
            mock_image_detail = {
                "id": image_id,
                "url": "https://res.cloudinary.com/dvbqtwgko/image/upload/v1752310322/obscura_images/file_del4l6.png",
                "thumbnail_url": "https://res.cloudinary.com/dvbqtwgko/image/upload/v1752310322/obscura_images/file_del4l6.png",
                "description": "Tree Observatory Location",
                "created_at": datetime.now().isoformat(),
                "prediction": {
                    "id": image_id + 1,
                    "input_data": {
                        "temperature": 22.5,
                        "humidity": 65.0,
                        "location": "Tree Observatory Location",
                        "timestamp": datetime.now().isoformat()
                    },
                    "result_data": {
                        "temperature": 23.8,
                        "humidity": 68.2,
                        "confidence": 0.87,
                        "climate_type": "temperate",
                        "vegetation_index": 0.73,
                        "predictions": {
                            "short_term": "Moderate warming expected",
                            "long_term": "Stable climate conditions"
                        }
                    },
                    "prompt": "Generate environmental vision based on current climate data",
                    "location": "Tree Observatory Location"
                }
            }
            
            return jsonify({
                "success": True,
                "image": mock_image_detail,
                "timestamp": datetime.now().isoformat(),
                "mode": "mock_data_for_local_development"
            }), 200
        
        # å…¶ä»–æ•°æ®åº“é”™è¯¯
        return jsonify({
            "success": False,
            "error": "Failed to fetch image detail",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/<int:image_id>/analysis', methods=['GET'])
def get_image_analysis(image_id):
    """
    è·å–å›¾ç‰‡è¯¦ç»†åˆ†ææ•°æ®APIç«¯ç‚¹
    
    è¿”å›: å›¾ç‰‡çš„æ·±åº¦åˆ†ææ•°æ®ï¼ŒåŒ…æ‹¬ç¯å¢ƒé¢„æµ‹ã€ç½®ä¿¡åº¦ç­‰
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # æŸ¥è¯¢å›¾ç‰‡å’Œå®Œæ•´é¢„æµ‹æ•°æ®
        cur.execute("""
            SELECT 
                i.id, i.url, i.thumbnail_url, i.description, i.created_at,
                p.id as prediction_id, p.input_data, p.result_data, p.prompt, p.location, p.created_at as prediction_created_at
            FROM images i
            LEFT JOIN predictions p ON i.prediction_id = p.id
            WHERE i.id = %s
        """, (image_id,))
        
        row = cur.fetchone()
        if not row:
            return jsonify({
                "success": False,
                "error": "Image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        # è§£æé¢„æµ‹ç»“æœæ•°æ®
        result_data = row[7] if row[7] else {}
        input_data = row[6] if row[6] else {}
        
        # æ„å»ºåˆ†æå“åº”æ•°æ®
        analysis_data = {
            "image": {
                "id": row[0],
                "url": row[1],
                "thumbnail_url": row[2],
                "description": row[3],
                "created_at": row[4].isoformat(),
                "file_size": _estimate_file_size(row[1]),
                "resolution": _estimate_resolution(row[1])
            },
            "prediction": {
                "id": row[5],
                "prompt": row[8],
                "location": row[9],
                "created_at": row[10].isoformat() if row[10] else None,
                "processing_time": _calculate_processing_time(row[4], row[10]) if row[10] else None
            } if row[5] else None,
            "analysis": {
                "environment_type": _extract_environment_type(result_data),
                "climate_prediction": _extract_climate_data(result_data),
                "vegetation_index": _extract_vegetation_data(result_data),
                "urban_development": _extract_urban_data(result_data),
                "confidence_scores": _extract_confidence_scores(result_data),
                "technical_details": _extract_technical_details(input_data, result_data)
            },
            "visualization_data": {
                "trend_data": _generate_trend_data(result_data),
                "confidence_chart": _generate_confidence_chart_data(result_data),
                "process_flow": _generate_process_flow_data()
            }
        }
        
        cur.close()
        conn.close()
        
        return jsonify({
            "success": True,
            "analysis": analysis_data,
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error fetching image analysis: {e}")
        return jsonify({
            "success": False,
            "error": "Failed to fetch image analysis",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/<int:image_id>/shap-test', methods=['GET'])
def test_shap_endpoint(image_id):
    """
    ç®€åŒ–çš„SHAPæµ‹è¯•ç«¯ç‚¹
    """
    try:
        return jsonify({
            "success": True,
            "message": "SHAP test endpoint working",
            "image_id": image_id,
            "timestamp": datetime.now().isoformat()
        }), 200
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/<int:image_id>/shap-analysis', methods=['GET'])
def get_image_shap_analysis(image_id):
    """
    è·å–å›¾ç‰‡çš„SHAPåˆ†ææ•°æ®APIç«¯ç‚¹
    
    è¿”å›: åŸºäºå›¾ç‰‡å…³è”çš„ç¯å¢ƒæ•°æ®è¿›è¡Œçš„SHAPåˆ†æç»“æœ
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # é¦–å…ˆå°è¯•è·å–å­˜å‚¨çš„åˆ†æç»“æœ
        cur.execute("""
            SELECT shap_data, ai_story, generated_at 
            FROM image_analysis 
            WHERE image_id = %s
        """, (image_id,))
        
        analysis_row = cur.fetchone()
        
        if analysis_row:
            # è¿”å›å­˜å‚¨çš„åˆ†æç»“æœ
            shap_data, ai_story, generated_at = analysis_row
            
            logger.info(f"âœ… Retrieved stored analysis for image {image_id}")
            
            cur.close()
            conn.close()
            
            # è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„
            enhanced_shap_analysis = transform_to_hierarchical_shap_data(json.loads(shap_data))
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            validation_result = validate_hierarchical_shap_data(enhanced_shap_analysis)
            
            return jsonify({
                "success": True,
                "data": {
                    **enhanced_shap_analysis,
                    'ai_story': json.loads(ai_story),
                    "integration_metadata": {
                        "analysis_timestamp": generated_at.isoformat(),
                        "model_version": "hierarchical_v1.1",
                        "analysis_source": "stored_analysis",
                        "note": "Pre-generated SHAP analysis with hierarchical features",
                        "data_format_version": "1.1.0"
                    },
                    "data_validation": validation_result
                },
                "timestamp": datetime.now().isoformat(),
                "mode": "stored_analysis"
            }), 200
        
        # å¦‚æœæ²¡æœ‰å­˜å‚¨çš„åˆ†æç»“æœï¼ŒæŸ¥è¯¢å›¾ç‰‡å…³è”çš„é¢„æµ‹æ•°æ®
        cur.execute("""
            SELECT 
                i.id, i.url, i.description,
                p.input_data, p.result_data, p.location
            FROM images i
            LEFT JOIN predictions p ON i.prediction_id = p.id
            WHERE i.id = %s
        """, (image_id,))
        
        row = cur.fetchone()
        if not row:
            return jsonify({
                "success": False,
                "error": "Image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        # æå–ç¯å¢ƒæ•°æ®ç”¨äºSHAPåˆ†æ
        input_data = row[3] if row[3] else {}
        result_data = row[4] if row[4] else {}
        location = row[5] if row[5] else "Unknown Location"
        
        cur.close()
        conn.close()
        
        # å›¾ç‰‡å­˜åœ¨ä½†æ²¡æœ‰åˆ†æç»“æœï¼Œç”Ÿæˆæ–°çš„åˆ†æ
        logger.info(f"ğŸ”„ No stored analysis found, generating new analysis for image {image_id}")
        
        # ç”Ÿæˆæ–°çš„SHAPåˆ†ææ•°æ®
        shap_data = generate_shap_analysis_data(image_id, row[2])
        
        # ç”ŸæˆAIæ•…äº‹
        story_data = generate_ai_environmental_story(shap_data)
        
        # è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„
        enhanced_shap_analysis = transform_to_hierarchical_shap_data(shap_data)
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        validation_result = validate_hierarchical_shap_data(enhanced_shap_analysis)
        
        # æ„å»ºå®Œæ•´çš„å“åº”æ•°æ®
        mock_shap_analysis = {
            **enhanced_shap_analysis,
            'ai_story': story_data,
            "integration_metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "model_version": "hierarchical_v1.1",
                "analysis_source": "generated_on_demand",
                "note": "Generated SHAP analysis with hierarchical features",
                "data_format_version": "1.1.0"
            },
            "data_validation": validation_result
        }
        
        return jsonify({
            "success": True,
            "data": mock_shap_analysis,
            "timestamp": datetime.now().isoformat(),
            "mode": "generated_on_demand"
        }), 200
        
    except Exception as e:
        logger.error(f"Error in SHAP analysis endpoint: {e}")
        
        # æ•°æ®åº“ä¸å¯ç”¨æ—¶ï¼Œæ£€æŸ¥æœ¬åœ°å­˜å‚¨æˆ–è¿”å›æ¨¡æ‹Ÿæ•°æ®
        if "nodename nor servname provided" in str(e) or "could not translate host name" in str(e):
            logger.info(f"Database unavailable - checking local storage for image {image_id}")
            
            # æ£€æŸ¥æœ¬åœ°åˆ†æå­˜å‚¨
            if image_id in LOCAL_ANALYSIS_STORE:
                stored_analysis = LOCAL_ANALYSIS_STORE[image_id]
                logger.info(f"âœ… Retrieved local analysis for image {image_id}")
                
                # è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„
                enhanced_shap_analysis = transform_to_hierarchical_shap_data(stored_analysis['shap_analysis'])
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                validation_result = validate_hierarchical_shap_data(enhanced_shap_analysis)
                
                return jsonify({
                    "success": True,
                    "data": {
                        **enhanced_shap_analysis,
                        'ai_story': stored_analysis['ai_story'],
                        "integration_metadata": {
                            "analysis_timestamp": stored_analysis['generated_at'],
                            "model_version": "hierarchical_v1.1",
                            "analysis_source": "local_storage",
                            "note": "Pre-generated SHAP analysis from local storage",
                            "data_format_version": "1.1.0"
                        },
                        "data_validation": validation_result
                    },
                    "timestamp": datetime.now().isoformat(),
                    "mode": "local_storage"
                }), 200
            
            # å¦‚æœæœ¬åœ°å­˜å‚¨ä¸­æ²¡æœ‰ï¼Œç”Ÿæˆæ–°çš„åˆ†æ
            logger.info(f"No local analysis found, generating new analysis for image {image_id}")
            
            # ç”Ÿæˆæ–°çš„SHAPåˆ†ææ•°æ®
            shap_data = generate_shap_analysis_data(image_id, "Tree Observatory Location")
            
            # ç”ŸæˆAIæ•…äº‹
            story_data = generate_ai_environmental_story(shap_data)
            
            # è½¬æ¢ä¸ºå±‚æ¬¡åŒ–ç»“æ„
            enhanced_fallback_data = transform_to_hierarchical_shap_data(shap_data)
            
            # éªŒè¯fallbackæ•°æ®å®Œæ•´æ€§
            fallback_validation = validate_hierarchical_shap_data(enhanced_fallback_data)
            
            return jsonify({
                "success": True,
                "data": {
                    **enhanced_fallback_data,
                    'ai_story': story_data,
                    "integration_metadata": {
                        "analysis_timestamp": datetime.now().isoformat(),
                        "model_version": "hierarchical_fallback_v1.1",
                        "analysis_source": "generated_for_local_development",
                        "note": "Generated SHAP analysis for local development",
                        "data_format_version": "1.1.0"
                    },
                    "data_validation": fallback_validation
                },
                "timestamp": datetime.now().isoformat(),
                "mode": "generated_for_local_development"
            }), 200
        
        return jsonify({
            "success": False,
            "error": "Failed to perform SHAP analysis",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/<int:image_id>/download', methods=['GET'])
def download_image(image_id):
    """
    å›¾ç‰‡ä¸‹è½½APIç«¯ç‚¹
    
    è¿”å›: é‡å®šå‘åˆ°Cloudinaryä¸‹è½½é“¾æ¥
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # æŸ¥è¯¢å›¾ç‰‡URL
        cur.execute("SELECT url, description FROM images WHERE id = %s", (image_id,))
        row = cur.fetchone()
        
        if not row:
            return jsonify({
                "success": False,
                "error": "Image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        image_url = row[0]
        description = row[1] or f"obscura_image_{image_id}"
        
        cur.close()
        conn.close()
        
        # ç”Ÿæˆå¸¦ä¸‹è½½å‚æ•°çš„Cloudinary URL
        download_url = image_url.replace('/upload/', '/upload/fl_attachment/')
        
        logger.info(f"Image download requested for ID: {image_id}")
        
        return jsonify({
            "success": True,
            "download_url": download_url,
            "filename": f"{description.replace(' ', '_')}.jpg",
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error generating download link: {e}")
        return jsonify({
            "success": False,
            "error": "Failed to generate download link",
            "timestamp": datetime.now().isoformat()
        }), 500

@images_bp.route('/navigation/<int:image_id>', methods=['GET'])
def get_image_navigation(image_id):
    """
    è·å–å›¾ç‰‡å¯¼èˆªä¿¡æ¯APIç«¯ç‚¹
    
    è¿”å›: ä¸Šä¸€å¼ å’Œä¸‹ä¸€å¼ å›¾ç‰‡çš„ä¿¡æ¯
    """
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # è·å–å½“å‰å›¾ç‰‡çš„åˆ›å»ºæ—¶é—´
        cur.execute("SELECT created_at FROM images WHERE id = %s", (image_id,))
        current_row = cur.fetchone()
        
        if not current_row:
            return jsonify({
                "success": False,
                "error": "Current image not found",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        current_created_at = current_row[0]
        
        # è·å–ä¸Šä¸€å¼ å›¾ç‰‡ï¼ˆæ›´æ—©çš„ï¼‰
        cur.execute("""
            SELECT id, url, thumbnail_url, description 
            FROM images 
            WHERE created_at < %s 
            ORDER BY created_at DESC 
            LIMIT 1
        """, (current_created_at,))
        prev_row = cur.fetchone()
        
        # è·å–ä¸‹ä¸€å¼ å›¾ç‰‡ï¼ˆæ›´æ™šçš„ï¼‰
        cur.execute("""
            SELECT id, url, thumbnail_url, description 
            FROM images 
            WHERE created_at > %s 
            ORDER BY created_at ASC 
            LIMIT 1
        """, (current_created_at,))
        next_row = cur.fetchone()
        
        cur.close()
        conn.close()
        
        navigation_data = {
            "current_id": image_id,
            "previous": {
                "id": prev_row[0],
                "url": prev_row[1],
                "thumbnail_url": prev_row[2],
                "description": prev_row[3]
            } if prev_row else None,
            "next": {
                "id": next_row[0],
                "url": next_row[1],
                "thumbnail_url": next_row[2],
                "description": next_row[3]
            } if next_row else None
        }
        
        return jsonify({
            "success": True,
            "navigation": navigation_data,
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error fetching navigation data: {e}")
        return jsonify({
            "success": False,
            "error": "Failed to fetch navigation data",
            "timestamp": datetime.now().isoformat()
        }), 500

# Helper functions for data processing
def _estimate_file_size(image_url):
    """ä¼°ç®—å›¾ç‰‡æ–‡ä»¶å¤§å°"""
    try:
        import requests
        response = requests.head(image_url, timeout=5)
        content_length = response.headers.get('Content-Length')
        if content_length:
            size_bytes = int(content_length)
            if size_bytes < 1024:
                return f"{size_bytes} B"
            elif size_bytes < 1024 * 1024:
                return f"{size_bytes / 1024:.1f} KB"
            else:
                return f"{size_bytes / (1024 * 1024):.1f} MB"
    except:
        pass
    return "Unknown"

def _estimate_resolution(image_url):
    """ä¼°ç®—å›¾ç‰‡åˆ†è¾¨ç‡"""
    try:
        # ä»Cloudinary URLä¸­æå–å˜æ¢ä¿¡æ¯ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
        return "1024x1024"  # é»˜è®¤åˆ†è¾¨ç‡
    except:
        return "Unknown"

def _calculate_processing_time(image_created, prediction_created):
    """è®¡ç®—å¤„ç†æ—¶é—´"""
    try:
        if image_created and prediction_created:
            delta = abs((image_created - prediction_created).total_seconds())
            if delta < 60:
                return f"{delta:.1f} seconds"
            else:
                return f"{delta / 60:.1f} minutes"
    except:
        pass
    return "Unknown"

def _extract_environment_type(result_data):
    """ä»ç»“æœæ•°æ®ä¸­æå–ç¯å¢ƒç±»å‹"""
    if not result_data:
        return {"type": "Unknown", "confidence": 0}
    
    # æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´
    return {
        "type": result_data.get("environment", "Mixed Environment"),
        "confidence": result_data.get("environment_confidence", 85.5)
    }

def _extract_climate_data(result_data):
    """æå–æ°”å€™é¢„æµ‹æ•°æ®"""
    if not result_data:
        return {"prediction": "Unknown", "confidence": 0}
    
    return {
        "prediction": result_data.get("climate", "Temperate"),
        "confidence": result_data.get("climate_confidence", 78.3)
    }

def _extract_vegetation_data(result_data):
    """æå–æ¤è¢«æŒ‡æ•°æ•°æ®"""
    if not result_data:
        return {"index": "Unknown", "confidence": 0}
    
    return {
        "index": result_data.get("vegetation", "Moderate"),
        "confidence": result_data.get("vegetation_confidence", 92.1)
    }

def _extract_urban_data(result_data):
    """æå–åŸå¸‚å‘å±•æ•°æ®"""
    if not result_data:
        return {"level": "Unknown", "confidence": 0}
    
    return {
        "level": result_data.get("urban_development", "Low"),
        "confidence": result_data.get("urban_confidence", 67.8)
    }

def _extract_confidence_scores(result_data):
    """æå–æ‰€æœ‰ç½®ä¿¡åº¦åˆ†æ•°"""
    if not result_data:
        return {}
    
    return {
        "overall": result_data.get("overall_confidence", 80.0),
        "environment": result_data.get("environment_confidence", 85.5),
        "climate": result_data.get("climate_confidence", 78.3),
        "vegetation": result_data.get("vegetation_confidence", 92.1),
        "urban": result_data.get("urban_confidence", 67.8)
    }

def _extract_technical_details(input_data, result_data):
    """æå–æŠ€æœ¯ç»†èŠ‚"""
    return {
        "model": {
            "architecture": "DALL-E 3",
            "version": "v3.0",
            "training_data": "GPT-4 Vision",
            "accuracy": "92.3%"
        },
        "processing": {
            "processing_time": "2.3 seconds",
            "memory_usage": "1.2 GB",
            "gpu_utilization": "78%",
            "batch_size": "1"
        },
        "environment": {
            "location": input_data.get("location", "Global"),
            "time_period": "2024",
            "climate_zone": "Temperate",
            "data_sources": "OpenWeather, Satellite Data"
        }
    }

def _generate_trend_data(result_data):
    """ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨æ•°æ®"""
    return {
        "labels": ["Jan", "Feb", "Mar", "Apr", "May", "Jun"],
        "datasets": [{
            "label": "Environmental Score",
            "data": [65, 69, 72, 78, 82, 85],
            "borderColor": "#CD853F",
            "backgroundColor": "rgba(205, 133, 63, 0.1)"
        }]
    }

def _generate_confidence_chart_data(result_data):
    """ç”Ÿæˆç½®ä¿¡åº¦å›¾è¡¨æ•°æ®"""
    confidence_scores = _extract_confidence_scores(result_data)
    return {
        "labels": ["Environment", "Climate", "Vegetation", "Urban"],
        "datasets": [{
            "label": "Confidence %",
            "data": [
                confidence_scores.get("environment", 85.5),
                confidence_scores.get("climate", 78.3),
                confidence_scores.get("vegetation", 92.1),
                confidence_scores.get("urban", 67.8)
            ],
            "backgroundColor": [
                "rgba(255, 191, 0, 0.8)",
                "rgba(205, 133, 63, 0.8)",
                "rgba(160, 82, 45, 0.8)",
                "rgba(139, 69, 19, 0.8)"
            ]
        }]
    }

def _generate_process_flow_data():
    """ç”Ÿæˆå¤„ç†æµç¨‹å›¾æ•°æ®"""
    return {
        "nodes": [
            {"id": "input", "label": "User Input", "x": 50, "y": 100},
            {"id": "weather", "label": "Weather API", "x": 200, "y": 50},
            {"id": "ai", "label": "AI Processing", "x": 350, "y": 100},
            {"id": "output", "label": "Image Generation", "x": 500, "y": 100}
        ],
        "edges": [
            {"from": "input", "to": "weather"},
            {"from": "input", "to": "ai"},
            {"from": "weather", "to": "ai"},
            {"from": "ai", "to": "output"}
        ]
    }
